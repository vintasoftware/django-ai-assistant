{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Django AI Assistant","text":"<p>Combine the power of Large Language Models with Django's productivity to build intelligent applications.</p> <p>Regardless of the feasibility of AGI, AI assistants are a new paradigm for computation. AI agents and assistants allow devs to easily build applications that make smart decisions.</p> <p>The latest LLMs from major AI providers have a \"killer feature\" called Tool Calling, which enables AI models to call methods from Django's side, and essentially do anything a Django view can, such as DB queries, file management, external API calls, etc.</p> <p>While users commonly interact with LLMs via conversations, AI Assistants can do a lot with any kind of string input, including JSON. Your end users won't even realize that a LLM is doing the heavy-lifting behind the scenes! Some ideas for innovative AI assistants include:</p> <ul> <li>A movie recommender chatbot that helps users manage their movie backlogs</li> <li>An autofill button for forms of your application</li> <li>Tailored email reminders that consider users' activity</li> <li>A real-time tourist guide that recommends attractions given the user's current location</li> </ul> <p>We provide examples for some of those applications. Get Started now!</p>"},{"location":"changelog/","title":"Changelog","text":"<p>This changelog references changes made both to the Django backend, <code>django-ai-assistant</code>, and the frontend TypeScript client, <code>django-ai-assistant-client</code>.</p> <p>Note</p> <p>The backend and the frontend are versioned together, that is, they have the same version number. When you update the backend, you should also update the frontend to the same version.</p>"},{"location":"changelog/#0.0.4","title":"0.0.4 July 5, 2024","text":"<ul> <li>Fix frontend README.</li> </ul>"},{"location":"changelog/#0.0.3","title":"0.0.3 July 5, 2024","text":"<ul> <li>Less restrictive Python version in pyproject.toml. Support future Python versions.</li> </ul>"},{"location":"changelog/#0.0.2","title":"0.0.2 June 28, 2024","text":"<ul> <li>Add support for Django 4.2 LTS</li> <li>Add support for Python 3.10 and 3.11</li> </ul>"},{"location":"changelog/#0.0.1","title":"0.0.1 June 25, 2024","text":"<ul> <li>Initial release</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We can always use your help to improve Django AI Assistant! Please feel free to tackle existing issues. If you have a new idea, please create a thread on Discussions.</p> <p>Please follow this guide to learn more about how to develop and test the project locally, before opening a pull request.</p>"},{"location":"contributing/#local-dev-setup","title":"Local Dev Setup","text":""},{"location":"contributing/#clone-the-repo","title":"Clone the repo","text":"<pre><code>git clone git@github.com:vintasoftware/django-ai-assistant.git\n</code></pre>"},{"location":"contributing/#install-development-tools","title":"Install development tools","text":"<p>This project uses Poetry for dependency and virtual environment management.</p> <p>If you need to install the version of Python recommended for the project, you can use Pyenv.</p> <p>For installing Node, we recommend NVM.</p>"},{"location":"contributing/#install-dependencies","title":"Install dependencies","text":""},{"location":"contributing/#backend","title":"Backend","text":"<p>Go to the project root. To instantiate the virtual environment, run</p> <pre><code>poetry shell\n</code></pre> <p>Install the Python dependencies:</p> <pre><code>poetry install\n</code></pre> <p>If you encounter an error regarding the Python version required for the project, you can use pyenv to install the appropriate version based on .python-version:</p> <pre><code>pyenv install\n</code></pre>"},{"location":"contributing/#frontend","title":"Frontend","text":"<p>Go to the frontend directory and install the Node dependencies:</p> <pre><code>cd frontend\npnpm install\n</code></pre>"},{"location":"contributing/#install-pre-commit-hooks","title":"Install pre-commit hooks","text":"<pre><code>pre-commit install\n</code></pre> <p>It's critical to run the pre-commit hooks before pushing your code to follow the project's code style, and avoid linting errors.</p>"},{"location":"contributing/#updating-the-openapi-schema","title":"Updating the OpenAPI schema","text":"<p>It's critical to update the OpenAPI schema when you make changes to the <code>django_ai_assistant/api/views.py</code> or related files:</p> <pre><code>poetry run python manage.py generate_openapi_schema --output frontend/openapi_schema.json\nsh -c 'cd frontend &amp;&amp; pnpm run generate-client'\n</code></pre>"},{"location":"contributing/#developing-with-the-example-project","title":"Developing with the example project","text":"<p>Run the frontend project in <code>build:watch</code> mode:</p> <pre><code>cd frontend\npnpm run build:watch\n</code></pre> <p>Go to the example project, install the dependencies, and link the frontend project:</p> <pre><code>cd ..  # back to project root directory\ncd example\npnpm install\npnpm remove django-ai-assistant-client  # remove the distributed package to use the local one\npnpm link ../frontend\n</code></pre> <p>Then follow the instructions in the example README to run the example project.</p>"},{"location":"contributing/#tests","title":"Tests","text":"<p>Before running tests copy the <code>.env.example</code> file to <code>.env.tests</code>.</p> <pre><code>cp .env.example .env.tests\n</code></pre> <p>Run tests with:</p> <pre><code>poetry run pytest\n</code></pre> <p>The tests use <code>pytest-vcr</code> to record and replay HTTP requests to AI models.</p> <p>If you're implementing a new test that needs to call a real AI model, you need to set the <code>OPENAI_API_KEY</code> environment variable with a real API key in the <code>.env.tests</code> file.</p> <p>Then, you will run the tests in record mode:</p> <pre><code>poetry run pytest --record-mode=once\n</code></pre> <p>To run frontend tests:</p> <pre><code>cd frontend\npnpm run test\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We use mkdocs-material to generate the documentation from markdown files. Check the files in the <code>docs</code> directory.</p> <p>To run the documentation locally, you need to run:</p> <pre><code>poetry run mkdocs serve\n</code></pre>"},{"location":"contributing/#release","title":"Release","text":"<p>Info</p> <p>The backend and the frontend are versioned together, that is, they should have the same version number.</p> <p>To release and publish a new version, follow these steps:</p> <ol> <li>Update the version in <code>pyproject.toml</code>, <code>frontend/package.json</code> and <code>example/package.json</code>.</li> <li>Re-install the local version of the Python project: <code>poetry install</code></li> <li>In the project root, run <code>poetry run python manage.py generate_openapi_schema --output frontend/openapi_schema.json</code> to update the OpenAPI schema.</li> <li>Re-install the local version of the frontend project:</li> </ol> <pre><code>cd frontend\npnpm install\npnpm run build\n</code></pre> <ol> <li>In the frontend directory, run <code>pnpm run generate-client</code> to update the TypeScript client with the new OpenAPI schema.</li> <li>Update the changelog in <code>CHANGELOG.md</code>.</li> <li>Open a PR with the changes.</li> <li>Once the PR is merged, run the Release GitHub Action to create a draft release.</li> <li>Review the draft release, ensure the description has at least the associated changelog entry, and publish it.</li> <li>Once the review is published, the Publish GitHub Action will automatically run to publish the new version to PyPI and npm. Check the logs to ensure the publication was successful.</li> </ol>"},{"location":"frontend/","title":"Frontend","text":"<p>Django AI Assistant has a frontend TypeScript client to facilitate the integration with the Django backend.</p>","boost":2},{"location":"frontend/#installation","title":"Installation","text":"<p>Install the frontend client using pnpm:</p> <pre><code>pnpm install django-ai-assistant-client\n</code></pre>","boost":2},{"location":"frontend/#client-configuration","title":"Client Configuration","text":"<p>First, you'll need to check what base path you used when setting up the Django AI Assistant backend. The base path is the URL prefix that the Django AI Assistant API is served under. Below the base path would be <code>ai-assistant</code>:</p> myproject/urls.py<pre><code>from django.urls import include, path\n\nurlpatterns = [\n    path(\"ai-assistant/\", include(\"django_ai_assistant.urls\")),\n    ...\n]\n</code></pre> <p>Before using the frontend client, import the <code>configAIAssistant</code> and configure it with the base path. If you're using React, a good place to do this is in the <code>App.tsx</code> file:</p> example/assets/js/App.tsx<pre><code>import { configAIAssistant } from \"django-ai-assistant-client\";\nimport React from \"react\";\n\nconfigAIAssistant({ BASE: \"ai-assistant\" });\n</code></pre> <p>Note in the configuration above, the Django server and the frontend client are using the same base path. If you're using a different base path, make sure to adjust the configuration accordingly.</p> <p>Now you can use the frontend client to interact with the Django AI Assistant backend. Here's an example of how to create a message:</p> <pre><code>import { aiCreateThreadMessage } from \"django-ai-assistant-client\";\n\nawait aiCreateThreadMessage({\n    threadId: 1,\n    requestBody: {\n        assistant_id: 1,\n        message: \"What's the weather like today in NYC?\"\n    }\n});\n</code></pre>","boost":2},{"location":"frontend/#advanced-client-configuration","title":"Advanced Client Configuration","text":"<p>By default the frontend client is authenticated via cookies (<code>CREDENTIALS === 'include'</code>). You can configure the client differently. Below is the default config:</p> <pre><code>configAIAssistant({\n    // Base path of the Django AI Assistant API, can be a relative or full URL:\n    BASE: '',\n    // Credentials mode for fetch requests:\n    CREDENTIALS: 'include',\n    // Record&lt;string, unknown&gt; with headers to be sent with each request:\n    HEADERS: undefined,\n    // Basic authentication username:\n    USERNAME: undefined,\n    // Basic authentication password:\n    PASSWORD: undefined,\n    // Token for authentication:\n    TOKEN: undefined,\n});\n</code></pre>","boost":2},{"location":"frontend/#client-functions","title":"Client Functions","text":"<p>The frontend client provides the following functions:</p>","boost":2},{"location":"frontend/#ailistassistants","title":"<code>aiListAssistants</code>","text":"<p>List all assistants the user has access to. Param: none Return: a <code>Promise</code> that resolves to an <code>Array</code> of <code>Assistant</code>.</p>","boost":2},{"location":"frontend/#aigetassistant","title":"<code>aiGetAssistant</code>","text":"<p>Get an assistant by ID. Param: <code>{ assistantId: string }</code> Return: <code>Promise</code> that resolves to <code>Assistant</code>.</p>","boost":2},{"location":"frontend/#ailistthreads","title":"<code>aiListThreads</code>","text":"<p>List all threads the user has access to. Param: none Return: a <code>Promise</code> that resolves to an <code>Array</code> of <code>Thread</code>.</p>","boost":2},{"location":"frontend/#aicreatethread","title":"<code>aiCreateThread</code>","text":"<p>Create a new thread. Param: <code>{ requestBody: { name: string } }</code> Return: a <code>Promise</code> that resolves to a <code>Thread</code>.</p>","boost":2},{"location":"frontend/#aigetthread","title":"<code>aiGetThread</code>","text":"<p>Get a thread by ID. Param: <code>{ threadId: string }</code> Return: a <code>Promise</code> that resolves to a <code>Thread</code>.</p>","boost":2},{"location":"frontend/#aiupdatethread","title":"<code>aiUpdateThread</code>","text":"<p>Update a thread by ID. Param: <code>{ threadId: string, requestBody: { name: string, assistant_id: string } }</code> Return: a <code>Promise</code> that resolves to a <code>Thread</code>.</p>","boost":2},{"location":"frontend/#aideletethread","title":"<code>aiDeleteThread</code>","text":"<p>Delete a thread by ID. Param: <code>{ threadId: string }</code> Return: a <code>Promise</code> that resolves to <code>void</code>.</p>","boost":2},{"location":"frontend/#ailistthreadmessages","title":"<code>aiListThreadMessages</code>","text":"<p>List all messages in a thread. Param: <code>{ threadId: string }</code> Return: a <code>Promise</code> that resolves to an <code>Array</code> of <code>ThreadMessage</code>.</p>","boost":2},{"location":"frontend/#aicreatethreadmessage","title":"<code>aiCreateThreadMessage</code>","text":"<p>Create a new message in a thread. Param: <code>{ threadId: string, requestBody: { assistant_id: string, message: string } }</code> Return: a <code>Promise</code> that resolves to <code>void</code>.</p>","boost":2},{"location":"frontend/#aideletethreadmessage","title":"<code>aiDeleteThreadMessage</code>","text":"<p>Delete a message in a thread. Param: <code>{ threadId: string, messageId: string }</code> Return: a <code>Promise</code> that resolves to <code>void</code>.</p> <p>Note</p> <p>These functions correspond to the Django AI Assistant API endpoints. Make sure to read the API documentation to learn about permissions.</p>","boost":2},{"location":"frontend/#type-definitions","title":"Type definitions","text":"<p>The type definitions are available at <code>frontend/src/client/types.gen.ts</code>. You can import the schemas directly from <code>django-ai-assistant-client</code> root:</p> <pre><code>import {\n    Assistant,\n    Thread,\n    ThreadMessage\n} from \"django-ai-assistant-client\";\n</code></pre>","boost":2},{"location":"frontend/#react-hooks","title":"React Hooks","text":"<p>The frontend client also provides React hooks to streamline application building.</p> <p>Warning</p> <p>You still have to call <code>configAIAssistant</code> on your application before using the hooks.</p>","boost":2},{"location":"frontend/#useassistantlist","title":"<code>useAssistantList</code>","text":"<p>React hook to manage the list of Assistants. Use like this:</p> <pre><code>import { useAssistantList } from \"django-ai-assistant-client\";\n\nexport function MyComponent() {\n    const {\n        assistants,\n        fetchAssistants,\n        loadingFetchAssistants\n    } = useAssistantList();\n    // ...\n}\n</code></pre>","boost":2},{"location":"frontend/#useassistant","title":"<code>useAssistant</code>","text":"<p>React hook to manage a single Assistant. Use like this:</p> <pre><code>import { useAssistant } from \"django-ai-assistant-client\";\n\nexport function MyComponent() {\n    const {\n        assistant,\n        fetchAssistant,\n        loadingFetchAssistant\n    } = useAssistant();\n    // ...\n}\n</code></pre>","boost":2},{"location":"frontend/#usethreadlist","title":"<code>useThreadList</code>","text":"<p>React hook to manage the list, create, and delete of Threads. Use like this:</p> <pre><code>import { useThreadList } from \"django-ai-assistant-client\";\n\nexport function MyComponent() {\n    const {\n        threads,\n        fetchThreads,\n        createThread,\n        updateThread,\n        deleteThread,\n        loadingFetchThreads,\n        loadingCreateThread,\n        loadingUpdateThread,\n        loadingDeleteThread\n    } = useThreadList();\n    // ...\n}\n</code></pre>","boost":2},{"location":"frontend/#usemessagelist","title":"<code>useMessageList</code>","text":"<p>React hook to manage the list, create, and delete of Messages. Use like this:</p> <pre><code>import { useMessageList, Thread } from \"django-ai-assistant-client\";\n\nexport function MyComponent() {\n    const [activeThread, setActiveThread] = useState&lt;Thread | null&gt;(null);\n    const {\n        messages,\n        fetchMessages,\n        createMessage,\n        deleteMessage,\n        loadingFetchMessages,\n        loadingCreateMessage,\n        loadingDeleteMessage\n    } = useMessageList({ threadId: activeThread?.id });\n    // ...\n}\n</code></pre>","boost":2},{"location":"frontend/#example-project","title":"Example project","text":"<p>The example project makes good use of the React hooks to build LLM-powered applications. Make sure to check it out!</p>","boost":2},{"location":"get-started/","title":"Get started","text":"","boost":2},{"location":"get-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python: </li> <li>Django: </li> </ul>","boost":2},{"location":"get-started/#how-to-install","title":"How to install","text":"<p>Install Django AI Assistant package:</p> <pre><code>pip install django-ai-assistant\n</code></pre> <p>Add Django AI Assistant to your Django project's <code>INSTALLED_APPS</code>:</p> myproject/settings.py<pre><code>INSTALLED_APPS = [\n    ...\n    'django_ai_assistant',\n    ...\n]\n</code></pre> <p>Run the migrations:</p> <pre><code>python manage.py migrate\n</code></pre> <p>Learn how to use the package in the Tutorial section.</p>","boost":2},{"location":"support/","title":"Support","text":"<p>If you have any questions or need help, feel free to create a thread on GitHub Discussions.</p> <p>In case you're facing a bug, please check existing issues and create a new one if needed.</p>"},{"location":"support/#commercial-support","title":"Commercial Support","text":"<p>This is an open-source project maintained by Vinta Software. We are always looking for exciting work! If you need any commercial support, feel free to get in touch: contact@vinta.com.br</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>In this tutorial, you will learn how to use Django AI Assistant to supercharge your Django project with LLM capabilities.</p>","boost":2},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<p>Make sure you properly configured Django AI Assistant as described in the Get Started guide.</p>","boost":2},{"location":"tutorial/#setting-up-api-keys","title":"Setting up API keys","text":"<p>The tutorial below uses OpenAI's gpt-4o model, so make sure you have <code>OPENAI_API_KEY</code> set as an environment variable for your Django project. You can also use other models, keep reading to learn more. Just make sure their keys are properly set.</p> <p>Note</p> <p>An easy way to set environment variables is to use a <code>.env</code> file in your project's root directory and use <code>python-dotenv</code> to load them. Our example project uses this approach.</p>","boost":2},{"location":"tutorial/#what-ai-assistants-can-do","title":"What AI Assistants can do","text":"<p>AI Assistants are LLMs that can answer to user queries as ChatGPT does, i.e. inputting and outputting strings. But when integrated with Django, they can also do anything a Django view can, such as accessing the database, checking permissions, sending emails, downloading and uploading media files, etc. This is possible by defining \"tools\" the AI can use. These tools are methods in an AI Assistant class on the Django side.</p>","boost":2},{"location":"tutorial/#defining-an-ai-assistant","title":"Defining an AI Assistant","text":"","boost":2},{"location":"tutorial/#registering","title":"Registering","text":"<p>To create an AI Assistant, you need to:</p> <ol> <li>Create an <code>ai_assistants.py</code> file;</li> <li>Define a class that inherits from <code>AIAssistant</code>;</li> <li>Provide an <code>id</code>, a <code>name</code>, some <code>instructions</code> for the LLM (a system prompt), and a <code>model</code> name:</li> </ol> myapp/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant\n\nclass WeatherAIAssistant(AIAssistant):\n    id = \"weather_assistant\"\n    name = \"Weather Assistant\"\n    instructions = \"You are a weather bot.\"\n    model = \"gpt-4o\"\n</code></pre>","boost":2},{"location":"tutorial/#defining-tools","title":"Defining tools","text":"<p>Useful tools give abilities the LLM doesn't have out-of-the-box, such as getting the current date and finding the current weather by calling an API.</p> <p>Use the <code>@method_tool</code> decorator to define a tool method in the AI Assistant:</p> myapp/ai_assistants.py<pre><code>from django.utils import timezone\nfrom django_ai_assistant import AIAssistant, method_tool\nimport json\n\nclass WeatherAIAssistant(AIAssistant):\n    id = \"weather_assistant\"\n    name = \"Weather Assistant\"\n    instructions = \"You are a weather bot.\"\n    model = \"gpt-4o\"\n\n    def get_instructions(self):\n        return f\"{self.instructions} Today is {timezone.now().isoformat()}.\"\n\n    @method_tool\n    def get_weather(self, location: str) -&gt; str:\n        \"\"\"Fetch the current weather data for a location\"\"\"\n        return json.dumps({\n            \"location\": location,\n            \"temperature\": \"25\u00b0C\",\n            \"weather\": \"sunny\"\n        })  # imagine some weather API here, this is just a placeholder\n</code></pre> <p>The <code>get_weather</code> method is a tool that the AI Assistant can use to get the current weather for a location, when the user asks for it. The tool method must be fully type-hinted (all parameters and return value), and it must include a descriptive docstring. This is necessary for the LLM model to understand the tool's purpose.</p> <p>A conversation with this Weather Assistant looks like this:</p> <pre><code>User: What's the weather in New York City?\nAI: The weather in NYC is sunny with a temperature of 25\u00b0C.\n</code></pre> <p>Note</p> <p>State of the art models such as gpt-4o can process JSON well. You can return a <code>json.dumps(api_output)</code> from a tool method and the model will be able to process it before responding the user.</p>","boost":2},{"location":"tutorial/#tool-parameters","title":"Tool parameters","text":"<p>It's possible to define more complex parameters for tools. As long as they're JSON serializable, the underlying LLM model should be able to call tools with the right arguments.</p> <p>In the <code>MovieRecommendationAIAssistant</code> from the example project, we have a <code>reorder_backlog</code> tool method that receives a list of IMDb URLs that represent the user's movie backlog order. Note the <code>Sequence[str]</code> parameter:</p> example/movies/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant, method_tool\n\nclass MovieRecommendationAIAssistant(AIAssistant):\n    ...\n\n    @method_tool\n    def reorder_backlog(self, imdb_url_list: Sequence[str]) -&gt; str:\n        \"\"\"Reorder movies in user's backlog.\"\"\"\n        ...\n</code></pre> <p>In <code>WeatherAIAssistant</code>, another assistant from the example project, we have a <code>fetch_forecast_weather</code> method tool with a <code>args_schema</code> parameter that defines a JSON schema for the tool arguments:</p> example/weather/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant, method_tool, BaseModel, Field\n\nclass WeatherAIAssistant(AIAssistant):\n    ...\n\n    class FetchForecastWeatherInput(BaseModel):\n        location: str = Field(description=\"Location to fetch the forecast weather for\")\n        forecast_date: date = Field(description=\"Date in the format 'YYYY-MM-DD'\")\n\n    @method_tool(args_schema=FetchForecastWeatherInput)\n    def fetch_forecast_weather(self, location, forecast_date) -&gt; dict:\n        \"\"\"Fetch the forecast weather data for a location\"\"\"\n        # forecast_date is a `date` object here\n        ...\n</code></pre> <p>Note</p> <p>It's important to provide a <code>description</code> for each field from <code>args_schema</code>. This improves the LLM's understanding of the tool's arguments.</p>","boost":2},{"location":"tutorial/#using-django-logic-in-tools","title":"Using Django logic in tools","text":"<p>You have access to the current request user in tools:</p> myapp/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant, method_tool\n\nclass PersonalAIAssistant(AIAssistant):\n    id = \"personal_assistant\"\n    name = \"Personal Assistant\"\n    instructions = \"You are a personal assistant.\"\n    model = \"gpt-4o\"\n\n    @method_tool\n    def get_current_user_username(self) -&gt; str:\n        \"\"\"Get the username of the current user\"\"\"\n        return self._user.username\n</code></pre> <p>You can also add any Django logic to tools, such as querying the database:</p> myapp/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant, method_tool\nimport json\n\nclass IssueManagementAIAssistant(AIAssistant):\n    id = \"issue_mgmt_assistant\"\n    name = \"Issue Management Assistant\"\n    instructions = \"You are an issue management bot.\"\n    model = \"gpt-4o\"\n\n    @method_tool\n    def get_current_user_assigned_issues(self) -&gt; str:\n        \"\"\"Get the issues assigned to the current user\"\"\"\n        return json.dumps({\n            \"issues\": list(Issue.objects.filter(assignee=self._user).values())\n        })\n</code></pre> <p>Warning</p> <p>Make sure you only return to the LLM what the user can see, considering permissions and privacy. Code the tools as if they were Django views.</p>","boost":2},{"location":"tutorial/#using-pre-implemented-tools","title":"Using pre-implemented tools","text":"<p>Django AI Assistant works with any LangChain-compatible tool. Just override the <code>get_tools</code> method in your AI Assistant class to include the tools you want to use.</p> <p>For example, you can use the <code>TavilySearch</code> tool to provide your AI Assistant with the ability to search the web for information about upcoming movies.</p> <p>First install dependencies:</p> <pre><code>pip install -U langchain-community tavily-python\n</code></pre> <p>Then, set the <code>TAVILY_API_KEY</code> environment variable. You'll need to sign up at Tavily.</p> <p>Finally, add the tool to your AI Assistant class by overriding the <code>get_tools</code> method:</p> myapp/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nclass MovieSearchAIAssistant(AIAssistant):\n    id = \"movie_search_assistant\"  # noqa: A003\n    instructions = (\n        \"You're a helpful movie search assistant. \"\n        \"Help the user find more information about movies. \"\n        \"Use the provided tools to search the web for upcoming movies. \"\n    )\n    name = \"Movie Search Assistant\"\n    model = \"gpt-4o\"\n\n    def get_instructions(self):\n        return f\"{self.instructions} Today is {timezone.now().isoformat()}.\"\n\n    def get_tools(self):\n        return [\n            TavilySearchResults(),\n            *super().get_tools(),\n        ]\n</code></pre> <p>Note</p> <p>As of now, Django AI Assistant is powered by LangChain, but previous knowledge on LangChain is NOT necessary to use this library, at least for the main use cases.</p>","boost":2},{"location":"tutorial/#using-an-ai-assistant","title":"Using an AI Assistant","text":"","boost":2},{"location":"tutorial/#manually-calling-an-ai-assistant","title":"Manually calling an AI Assistant","text":"<p>You can manually call an AI Assistant from anywhere in your Django application:</p> <pre><code>from myapp.ai_assistants import WeatherAIAssistant\n\nassistant = WeatherAIAssistant()\noutput = assistant.run(\"What's the weather in New York City?\")\nassert output == \"The weather in NYC is sunny with a temperature of 25\u00b0C.\"\n</code></pre> <p>The constructor of <code>AIAssistant</code> receives <code>user</code>, <code>request</code>, <code>view</code> as optional parameters, which can be used in the tools with <code>self._user</code>, <code>self._request</code>, <code>self._view</code>. Also, any extra parameters passed in constructor are stored at <code>self._init_kwargs</code>.</p>","boost":2},{"location":"tutorial/#threads-of-messages","title":"Threads of Messages","text":"<p>The django-ai-assistant app provides two models <code>Thread</code> and <code>Message</code> to store and retrieve conversations with AI Assistants. LLMs are stateless by design, meaning they don't hold any context between calls. All they know is the current input. But by using the <code>AIAssistant</code> class, the conversation state is stored in the database as multiple <code>Message</code> of a <code>Thread</code>, and automatically retrieved then passed to the LLM when calling the AI Assistant.</p> <p>To create a <code>Thread</code>, you can use a helper from the <code>django_ai_assistant.use_cases</code> module. For example:</p> <pre><code>from django_ai_assistant.use_cases import create_thread, get_thread_messages\nfrom myapp.ai_assistants import WeatherAIAssistant\n\nthread = create_thread(name=\"Weather Chat\", user=user)\nassistant = WeatherAIAssistant()\nassistant.run(\"What's the weather in New York City?\", thread_id=thread.id)\n\nmessages = get_thread_messages(thread=thread, user=user)  # returns both user and AI messages\n</code></pre> <p>More CRUD helpers are available at <code>django_ai_assistant.use_cases</code> module. Check the Reference for more information.</p>","boost":2},{"location":"tutorial/#using-built-in-api-views","title":"Using built-in API views","text":"<p>You can use the built-in API views to interact with AI Assistants via HTTP requests from any frontend, such as a React application or a mobile app. Add the following to your Django project's <code>urls.py</code>:</p> myproject/urls.py<pre><code>from django.urls import include, path\n\nurlpatterns = [\n    path(\"ai-assistant/\", include(\"django_ai_assistant.urls\")),\n    ...\n]\n</code></pre> <p>The built-in API supports retrieval of Assistants info, as well as CRUD for Threads and Messages. It has a OpenAPI schema that you can explore at <code>http://localhost:8000/ai-assistant/docs</code>, when running your project locally.</p>","boost":2},{"location":"tutorial/#configuring-the-api","title":"Configuring the API","text":"<p>The built-in API is implemented using Django Ninja. By default, it is initialized with the following setting:</p> myproject/settings.py<pre><code>AI_ASSISTANT_INIT_API_FN = \"django_ai_assistant.api.views.init_api\"\n</code></pre> <p>You can override this setting in your Django project's <code>settings.py</code> to customize the API, such as using a different authentication method or modifying other configurations.</p> <p>The method signature for <code>AI_ASSISTANT_INIT_API_FN</code> is as follows:</p> <pre><code>from ninja import NinjaAPI\n\ndef init_api():\n    return NinjaAPI(...)\n</code></pre> <p>By providing your own implementation of <code>init_api</code>, you can tailor the API setup to better fit your project's requirements.</p>","boost":2},{"location":"tutorial/#configuring-permissions","title":"Configuring permissions","text":"<p>The API uses the helpers from the <code>django_ai_assistant.use_cases</code> module, which have permission checks to ensure the user can use a certain AI Assistant or do CRUD on Threads and Messages.</p> <p>By default, any authenticated user can use any AI Assistant, and create a thread. Users can manage both their own threads and the messages on them. Therefore, the default permissions are:</p> myproject/settings.py<pre><code>AI_ASSISTANT_CAN_CREATE_THREAD_FN = \"django_ai_assistant.permissions.allow_all\"\nAI_ASSISTANT_CAN_VIEW_THREAD_FN = \"django_ai_assistant.permissions.owns_thread\"\nAI_ASSISTANT_CAN_UPDATE_THREAD_FN = \"django_ai_assistant.permissions.owns_thread\"\nAI_ASSISTANT_CAN_DELETE_THREAD_FN = \"django_ai_assistant.permissions.owns_thread\"\nAI_ASSISTANT_CAN_CREATE_MESSAGE_FN = \"django_ai_assistant.permissions.owns_thread\"\nAI_ASSISTANT_CAN_UPDATE_MESSAGE_FN = \"django_ai_assistant.permissions.owns_thread\"\nAI_ASSISTANT_CAN_DELETE_MESSAGE_FN = \"django_ai_assistant.permissions.owns_thread\"\nAI_ASSISTANT_CAN_RUN_ASSISTANT = \"django_ai_assistant.permissions.allow_all\"\n</code></pre> <p>You can override these settings in your Django project's <code>settings.py</code> to customize the permissions.</p> <p>Thread permission signatures look like this:</p> <pre><code>from django_ai_assistant.models import Thread\nfrom django.http import HttpRequest\n\ndef check_custom_thread_permission(\n        thread: Thread,\n        user: Any,\n        request: HttpRequest | None = None) -&gt; bool:\n    return ...\n</code></pre> <p>While Message permission signatures look like this:</p> <pre><code>from django_ai_assistant.models import Thread, Message\nfrom django.http import HttpRequest\n\ndef check_custom_message_permission(\n        message: Message,\n        thread: Thread,\n        user: Any,\n        request: HttpRequest | None = None) -&gt; bool:\n    return ...\n</code></pre>","boost":2},{"location":"tutorial/#frontend-integration","title":"Frontend integration","text":"<p>You can integrate Django AI Assistant with frontend frameworks like React or Vue.js. Please check the frontend documentation.</p> <p>If you want to use traditional Django templates, you can try using HTMX to avoid page refreshes. Check the example project, it includes a HTMX application.</p>","boost":2},{"location":"tutorial/#advanced-usage","title":"Advanced usage","text":"","boost":2},{"location":"tutorial/#using-other-ai-models","title":"Using other AI models","text":"<p>By default the supported models are OpenAI ones, but you can use any chat model from Langchain that supports Tool Calling by overriding <code>get_llm</code>:</p> myapp/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant\nfrom langchain_anthropic import ChatAnthropic\n\nclass WeatherAIAssistant(AIAssistant):\n    id = \"weather_assistant\"\n    name = \"Weather Assistant\"\n    instructions = \"You are a weather bot.\"\n    model = \"claude-3-opus-20240229\"\n\n    def get_llm(self):\n        model = self.get_model()\n        temperature = self.get_temperature()\n        model_kwargs = self.get_model_kwargs()\n        return ChatAnthropic(\n            model_name=model,\n            temperature=temperature,\n            model_kwargs=model_kwargs,\n            timeout=None,\n            max_retries=2,\n        )\n</code></pre>","boost":2},{"location":"tutorial/#composing-ai-assistants","title":"Composing AI Assistants","text":"<p>One AI Assistant can call another AI Assistant as a tool. This is useful for composing complex AI Assistants. Use the <code>as_tool</code> method for that:</p> myapp/ai_assistants.py<pre><code>class SimpleAssistant(AIAssistant):\n    ...\n\nclass AnotherSimpleAssistant(AIAssistant):\n    ...\n\nclass ComplexAssistant(AIAssistant):\n    ...\n\n    def get_tools(self) -&gt; Sequence[BaseTool]:\n        return [\n            SimpleAssistant().as_tool(\n                description=\"Tool to &lt;...add a meaningful description here...&gt;\"),\n            AnotherSimpleAssistant().as_tool(\n                description=\"Tool to &lt;...add a meaningful description here...&gt;\"),\n            *super().get_tools(),\n        ]\n</code></pre> <p>The <code>movies/ai_assistants.py</code> file in the example project shows an example of a composed AI Assistant that's able to recommend movies and manage the user's movie backlog.</p>","boost":2},{"location":"tutorial/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>You can use RAG in your AI Assistants. RAG means using a retriever to fetch chunks of textual data from a pre-existing DB to give context to the LLM. This means the LLM will have access to a context your retriever logic provides when generating the response, thereby improving the quality of the response by avoiding generic or off-topic answers.</p> <p>For this to work, your must do the following in your AI Assistant:</p> <ol> <li>Add <code>has_rag = True</code> as a class attribute;</li> <li>Override the <code>get_retriever</code> method to return a Langchain Retriever.</li> </ol> <p>For example:</p> myapp/ai_assistants.py<pre><code>from django_ai_assistant import AIAssistant\n\nclass DocsAssistant(AIAssistant):\n    id = \"docs_assistant\"  # noqa: A003\n    name = \"Docs Assistant\"\n    instructions = (\n        \"You are an assistant for answering questions related to the provided context. \"\n        \"Use the following pieces of retrieved context to answer the user's question. \"\n    )\n    model = \"gpt-4o\"\n    has_rag = True\n\n    def get_retriever(self) -&gt; BaseRetriever:\n        return ...  # use a Langchain Retriever here\n</code></pre> <p>The <code>rag/ai_assistants.py</code> file in the example project shows an example of a RAG-powered AI Assistant that's able to answer questions about Django using the Django Documentation as context.</p>","boost":2},{"location":"tutorial/#support-for-other-types-of-primary-key-pk","title":"Support for other types of Primary Key (PK)","text":"<p>You can have Django AI Assistant models use other types of primary key, such as strings, UUIDs, etc. This is useful if you're concerned about leaking IDs that exponse thread count, message count, etc. to the frontend. When using UUIDs, it will prevent users from figuring out if a thread or message exist or not (due to HTTP 404 vs 403).</p> <p>Here are the files you have to change if you need the ids to be UUID:</p> myapp/fields.py<pre><code>import uuid\nfrom django.db.backends.base.operations import BaseDatabaseOperations\nfrom django.db.models import AutoField, UUIDField\n\nBaseDatabaseOperations.integer_field_ranges['UUIDField'] = (0, 0)\n\nclass UUIDAutoField(UUIDField, AutoField):\n    def __init__(self, *args, **kwargs):\n        kwargs.setdefault('default', uuid.uuid4)\n        kwargs.setdefault('editable', False)\n        super().__init__(*args, **kwargs)\n</code></pre> myapp/apps.py<pre><code>from django_ai_assistant.apps import AIAssistantConfig\n\nclass AIAssistantConfigOverride(AIAssistantConfig):\n    default_auto_field = \"django_ai_assistant.api.fields.UUIDAutoField\"\n</code></pre> myproject/settings.py<pre><code>INSTALLED_APPS = [\n    # \"django_ai_assistant\", remove this line and add the one below\n    \"example.apps.AIAssistantConfigOverride\",\n]\n</code></pre> <p>Make sure to run migrations after those changes:</p> <pre><code>python manage.py makemigrations\npython manage.py migrate\n</code></pre> <p>For more information, check Django docs on overriding AppConfig.</p>","boost":2},{"location":"tutorial/#further-configuration-of-ai-assistants","title":"Further configuration of AI Assistants","text":"<p>You can further configure the <code>AIAssistant</code> subclasses by overriding its public methods. Check the Reference for more information.</p>","boost":2},{"location":"reference/","title":"Reference","text":"<p>This is the reference documentation for the Django AI Assistant library.</p>"},{"location":"reference/#modules","title":"Modules","text":"<ul> <li>django_ai_assistant.helpers.use_cases</li> <li>django_ai_assistant.helpers.assistants</li> <li>django_ai_assistant.models</li> </ul>"},{"location":"reference/assistants-ref/","title":"django_ai_assistant.helpers.assistants","text":""},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant","title":"<code>AIAssistant</code>","text":"<p>Base class for AI Assistants. Subclasses must define at least the following attributes:</p> <ul> <li>id: str</li> <li>name: str</li> <li>instructions: str</li> <li>model: str</li> </ul> <p>Subclasses can override the public methods to customize the behavior of the assistant.</p> <p>Tools can be added to the assistant by decorating methods with <code>@method_tool</code>.</p> <p>Check the docs Tutorial for more info on how to build an AI Assistant.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>class AIAssistant(abc.ABC):  # noqa: F821\n    \"\"\"Base class for AI Assistants. Subclasses must define at least the following attributes:\n\n    * id: str\n    * name: str\n    * instructions: str\n    * model: str\n\n    Subclasses can override the public methods to customize the behavior of the assistant.\\n\n    Tools can be added to the assistant by decorating methods with `@method_tool`.\\n\n    Check the docs Tutorial for more info on how to build an AI Assistant.\n    \"\"\"\n\n    id: ClassVar[str]  # noqa: A003\n    \"\"\"Class variable with the id of the assistant. Used to select the assistant to use.\\n\n    Must be unique across the whole Django project and match the pattern '^[a-zA-Z0-9_-]+$'.\"\"\"\n    name: ClassVar[str]\n    \"\"\"Class variable with the name of the assistant.\n    Should be a friendly name to optionally display to users.\"\"\"\n    instructions: str\n    \"\"\"Instructions for the AI assistant knowing what to do. This is the LLM system prompt.\"\"\"\n    model: str\n    \"\"\"LLM model name to use for the assistant.\\n\n    Should be a valid model name from OpenAI, because the default `get_llm` method uses OpenAI.\\n\n    `get_llm` can be overridden to use a different LLM implementation.\n    \"\"\"\n    temperature: float = 1.0\n    \"\"\"Temperature to use for the assistant LLM model.\\nDefaults to `1.0`.\"\"\"\n    tool_max_concurrency: int = 1\n    \"\"\"Maximum number of tools to run concurrently / in parallel.\\nDefaults to `1` (no concurrency).\"\"\"\n    has_rag: bool = False\n    \"\"\"Whether the assistant uses RAG (Retrieval-Augmented Generation) or not.\\n\n    Defaults to `False`.\n    When True, the assistant will use a retriever to get documents to provide as context to the LLM.\n    Additionally, the assistant class should implement the `get_retriever` method to return\n    the retriever to use.\"\"\"\n    structured_output: Dict[str, Any] | Type[BaseModel] | Type | None = None\n    \"\"\"Structured output to use for the assistant.\\n\n    Defaults to `None`.\n    When not `None`, the assistant will return a structured output in the provided format.\n    See https://python.langchain.com/v0.2/docs/how_to/structured_output/ for the available formats.\n    \"\"\"\n    _user: Any | None\n    \"\"\"The current user the assistant is helping. A model instance.\\n\n    Set by the constructor.\n    When API views are used, this is set to the current request user.\\n\n    Can be used in any `@method_tool` to customize behavior.\"\"\"\n    _request: Any | None\n    \"\"\"The current Django request the assistant was initialized with. A request instance.\\n\n    Set by the constructor.\\n\n    Can be used in any `@method_tool` to customize behavior.\"\"\"\n    _view: Any | None\n    \"\"\"The current Django view the assistant was initialized with. A view instance.\\n\n    Set by the constructor.\\n\n    Can be used in any `@method_tool` to customize behavior.\"\"\"\n    _init_kwargs: dict[str, Any]\n    \"\"\"Extra keyword arguments passed to the constructor.\\n\n    Set by the constructor.\\n\n    Can be used in any `@method_tool` to customize behavior.\"\"\"\n    _method_tools: Sequence[BaseTool]\n    \"\"\"List of `@method_tool` tools the assistant can use. Automatically set by the constructor.\"\"\"\n\n    _registry: ClassVar[dict[str, type[\"AIAssistant\"]]] = {}\n    \"\"\"Registry of all AIAssistant subclasses by their id.\\n\n    Automatically populated by when a subclass is declared.\\n\n    Use `get_cls_registry` and `get_cls` to access the registry.\"\"\"\n\n    def __init__(self, *, user=None, request=None, view=None, **kwargs: Any):\n        \"\"\"Initialize the AIAssistant instance.\\n\n        Optionally set the current user, request, and view for the assistant.\\n\n        Those can be used in any `@method_tool` to customize behavior.\\n\n\n        Args:\n            user (Any | None): The current user the assistant is helping. A model instance.\n                Defaults to `None`. Stored in `self._user`.\n            request (Any | None): The current Django request the assistant was initialized with.\n                A request instance. Defaults to `None`. Stored in `self._request`.\n            view (Any | None): The current Django view the assistant was initialized with.\n                A view instance. Defaults to `None`. Stored in `self._view`.\n            **kwargs: Extra keyword arguments passed to the constructor. Stored in `self._init_kwargs`.\n        \"\"\"\n\n        self._user = user\n        self._request = request\n        self._view = view\n        self._init_kwargs = kwargs\n\n        self._set_method_tools()\n\n    def __init_subclass__(cls, **kwargs: Any):\n        \"\"\"Called when a class is subclassed from AIAssistant.\n\n        This method is automatically invoked when a new subclass of AIAssistant\n        is created. It allows AIAssistant to perform additional setup or configuration\n        for the subclass, such as registering the subclass in a registry.\n\n        Args:\n            cls (type): The newly created subclass.\n            **kwargs: Additional keyword arguments passed during subclass creation.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n\n        if not hasattr(cls, \"id\"):\n            raise AIAssistantMisconfiguredError(f\"Assistant id is not defined at {cls.__name__}\")\n        if cls.id is None:\n            raise AIAssistantMisconfiguredError(f\"Assistant id is None at {cls.__name__}\")\n        if not re.match(r\"^[a-zA-Z0-9_-]+$\", cls.id):\n            # id should match the pattern '^[a-zA-Z0-9_-]+$ to support as_tool in OpenAI\n            raise AIAssistantMisconfiguredError(\n                f\"Assistant id '{cls.id}' does not match the pattern '^[a-zA-Z0-9_-]+$'\"\n                f\"at {cls.__name__}\"\n            )\n\n        cls._registry[cls.id] = cls\n\n    def _set_method_tools(self):\n        # Find tool methods (decorated with `@method_tool` from django_ai_assistant/tools.py):\n        members = inspect.getmembers(\n            self,\n            predicate=lambda m: inspect.ismethod(m) and getattr(m, \"_is_tool\", False),\n        )\n        tool_methods = [m for _, m in members]\n\n        # Sort tool methods by the order they appear in the source code,\n        # since this can be meaningful:\n        tool_methods.sort(key=lambda m: inspect.getsourcelines(m)[1])\n\n        # Transform tool methods into tool objects:\n        tools = []\n        for method in tool_methods:\n            if hasattr(method, \"_tool_maker_args\"):\n                tool = tool_decorator(\n                    *method._tool_maker_args,\n                    **method._tool_maker_kwargs,\n                )(method)\n            else:\n                tool = tool_decorator(method)\n            tools.append(cast(BaseTool, tool))\n\n        # Remove self from each tool args_schema:\n        for tool in tools:\n            if tool.args_schema:\n                if isinstance(tool.args_schema.__fields_set__, set):\n                    tool.args_schema.__fields_set__.remove(\"self\")\n                tool.args_schema.__fields__.pop(\"self\", None)\n\n        self._method_tools = tools\n\n    @classmethod\n    def get_cls_registry(cls) -&gt; dict[str, type[\"AIAssistant\"]]:\n        \"\"\"Get the registry of AIAssistant classes.\n\n        Returns:\n            dict[str, type[AIAssistant]]: A dictionary mapping assistant ids to their classes.\n        \"\"\"\n        return cls._registry\n\n    @classmethod\n    def get_cls(cls, assistant_id: str) -&gt; type[\"AIAssistant\"]:\n        \"\"\"Get the AIAssistant class for the given assistant ID.\n\n        Args:\n            assistant_id (str): The ID of the assistant to get.\n        Returns:\n            type[AIAssistant]: The AIAssistant subclass for the given ID.\n        \"\"\"\n        return cls.get_cls_registry()[assistant_id]\n\n    @classmethod\n    def clear_cls_registry(cls: type[\"AIAssistant\"]) -&gt; None:\n        \"\"\"Clear the registry of AIAssistant classes.\"\"\"\n\n        cls._registry.clear()\n\n    def get_instructions(self) -&gt; str:\n        \"\"\"Get the instructions for the assistant. By default, this is the `instructions` attribute.\\n\n        Override the `instructions` attribute or this method to use different instructions.\n\n        Returns:\n            str: The instructions for the assistant, i.e., the LLM system prompt.\n        \"\"\"\n        return self.instructions\n\n    def get_model(self) -&gt; str:\n        \"\"\"Get the LLM model name for the assistant. By default, this is the `model` attribute.\\n\n        Used by the `get_llm` method to create the LLM instance.\\n\n        Override the `model` attribute or this method to use a different LLM model.\n\n        Returns:\n            str: The LLM model name for the assistant.\n        \"\"\"\n        return self.model\n\n    def get_temperature(self) -&gt; float:\n        \"\"\"Get the temperature to use for the assistant LLM model.\n        By default, this is the `temperature` attribute, which is `1.0` by default.\\n\n        Used by the `get_llm` method to create the LLM instance.\\n\n        Override the `temperature` attribute or this method to use a different temperature.\n\n        Returns:\n            float: The temperature to use for the assistant LLM model.\n        \"\"\"\n        return self.temperature\n\n    def get_model_kwargs(self) -&gt; dict[str, Any]:\n        \"\"\"Get additional keyword arguments to pass to the LLM model constructor.\\n\n        Used by the `get_llm` method to create the LLM instance.\\n\n        Override this method to pass additional keyword arguments to the LLM model constructor.\n\n        Returns:\n            dict[str, Any]: Additional keyword arguments to pass to the LLM model constructor.\n        \"\"\"\n        return {}\n\n    def get_llm(self) -&gt; BaseChatModel:\n        \"\"\"Get the Langchain LLM instance for the assistant.\n        By default, this uses the OpenAI implementation.\\n\n        `get_model`, `get_temperature`, and `get_model_kwargs` are used to create the LLM instance.\\n\n        Override this method to use a different LLM implementation.\n\n        Returns:\n            BaseChatModel: The LLM instance for the assistant.\n        \"\"\"\n        model = self.get_model()\n        temperature = self.get_temperature()\n        model_kwargs = self.get_model_kwargs()\n        return ChatOpenAI(\n            model=model,\n            temperature=temperature,\n            model_kwargs=model_kwargs,\n        )\n\n    def get_structured_output_llm(self) -&gt; Runnable:\n        \"\"\"Get the LLM model to use for the structured output.\n\n        Returns:\n            BaseChatModel: The LLM model to use for the structured output.\n        \"\"\"\n        if not self.structured_output:\n            raise ValueError(\"structured_output is not defined\")\n\n        llm = self.get_llm()\n\n        method = \"json_mode\"\n        if isinstance(llm, ChatOpenAI):\n            # When using ChatOpenAI, it's better to use json_schema method\n            # because it enables strict mode.\n            # https://platform.openai.com/docs/guides/structured-outputs\n            method = \"json_schema\"\n\n        return llm.with_structured_output(self.structured_output, method=method)\n\n    def get_tools(self) -&gt; Sequence[BaseTool]:\n        \"\"\"Get the list of method tools the assistant can use.\n        By default, this is the `_method_tools` attribute, which are all `@method_tool`s.\\n\n        Override and call super to add additional tools,\n        such as [any langchain_community tools](https://python.langchain.com/v0.2/docs/integrations/tools/).\n\n        Returns:\n            Sequence[BaseTool]: The list of tools the assistant can use.\n        \"\"\"\n        return self._method_tools\n\n    def get_document_separator(self) -&gt; str:\n        \"\"\"Get the RAG document separator to use in the prompt. Only used when `has_rag=True`.\\n\n        Defaults to `\"\\\\n\\\\n\"`, which is the Langchain default.\\n\n        Override this method to use a different separator.\n\n        Returns:\n            str: a separator for documents in the prompt.\n        \"\"\"\n        return DEFAULT_DOCUMENT_SEPARATOR\n\n    def get_document_prompt(self) -&gt; PromptTemplate:\n        \"\"\"Get the PromptTemplate template to use when rendering RAG documents in the prompt.\n        Only used when `has_rag=True`.\\n\n        Defaults to `PromptTemplate.from_template(\"{page_content}\")`, which is the Langchain default.\\n\n        Override this method to use a different template.\n\n        Returns:\n            PromptTemplate: a prompt template for RAG documents.\n        \"\"\"\n        return DEFAULT_DOCUMENT_PROMPT\n\n    def get_retriever(self) -&gt; BaseRetriever:\n        \"\"\"Get the RAG retriever to use for fetching documents.\\n\n        Must be implemented by subclasses when `has_rag=True`.\\n\n\n        Returns:\n            BaseRetriever: the RAG retriever to use for fetching documents.\n        \"\"\"\n        raise NotImplementedError(\n            f\"Override the get_retriever with your implementation at {self.__class__.__name__}\"\n        )\n\n    def get_contextualize_prompt(self) -&gt; ChatPromptTemplate:\n        \"\"\"Get the contextualize prompt template for the assistant.\\n\n        This is used when `has_rag=True` and there are previous messages in the thread.\n        Since the latest user question might reference the chat history,\n        the LLM needs to generate a new standalone question,\n        and use that question to query the retriever for relevant documents.\\n\n        By default, this is a prompt that asks the LLM to\n        reformulate the latest user question without the chat history.\\n\n        Override this method to use a different contextualize prompt.\\n\n        See `get_history_aware_retriever` for how this prompt is used.\\n\n\n        Returns:\n            ChatPromptTemplate: The contextualize prompt template for the assistant.\n        \"\"\"\n        contextualize_q_system_prompt = (\n            \"Given a chat history and the latest user question \"\n            \"which might reference context in the chat history, \"\n            \"formulate a standalone question which can be understood \"\n            \"without the chat history. Do NOT answer the question, \"\n            \"just reformulate it if needed and otherwise return it as is.\"\n        )\n        return ChatPromptTemplate.from_messages(\n            [\n                (\"system\", contextualize_q_system_prompt),\n                # TODO: make history key configurable?\n                MessagesPlaceholder(\"history\"),\n                # TODO: make input key configurable?\n                (\"human\", \"{input}\"),\n            ]\n        )\n\n    def get_history_aware_retriever(self) -&gt; Runnable[dict, RetrieverOutput]:\n        \"\"\"Get the history-aware retriever Langchain chain for the assistant.\\n\n        This is used when `has_rag=True` to fetch documents based on the chat history.\\n\n        By default, this is a chain that checks if there is chat history,\n        and if so, it uses the chat history to generate a new standalone question\n        to query the retriever for relevant documents.\\n\n        When there is no chat history, it just passes the input to the retriever.\\n\n        Override this method to use a different history-aware retriever chain.\n\n        Read more about the history-aware retriever in the\n        [Langchain docs](https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/).\n\n        Returns:\n            Runnable[dict, RetrieverOutput]: a history-aware retriever Langchain chain.\n        \"\"\"\n        llm = self.get_llm()\n        retriever = self.get_retriever()\n        prompt = self.get_contextualize_prompt()\n\n        # Based on create_history_aware_retriever:\n        return RunnableBranch(\n            (\n                lambda x: not x.get(\"history\", False),  # pyright: ignore[reportAttributeAccessIssue]\n                # If no chat history, then we just pass input to retriever\n                (lambda x: x[\"input\"]) | retriever,\n            ),\n            # If chat history, then we pass inputs to LLM chain, then to retriever\n            prompt | llm | StrOutputParser() | retriever,\n        )\n\n    @with_cast_id\n    def as_graph(self, thread_id: Any | None = None) -&gt; Runnable[dict, dict]:\n        \"\"\"Create the Langchain graph for the assistant.\\n\n        This graph is an agent that supports chat history, tool calling, and RAG (if `has_rag=True`).\\n\n        `as_graph` uses many other methods to create the graph for the assistant.\n        Prefer to override the other methods to customize the graph for the assistant.\n        Only override this method if you need to customize the graph at a lower level.\n\n        Args:\n            thread_id (Any | None): The thread ID for the chat message history.\n                If `None`, an in-memory chat message history is used.\n\n        Returns:\n            the compiled graph\n        \"\"\"\n        from django_ai_assistant.models import Thread\n\n        llm = self.get_llm()\n        tools = self.get_tools()\n        llm_with_tools = llm.bind_tools(tools) if tools else llm\n        if thread_id:\n            thread = Thread.objects.get(id=thread_id)\n        else:\n            thread = None\n\n        def custom_add_messages(left: list[BaseMessage], right: list[BaseMessage]):\n            result = add_messages(left, right)  # type: ignore\n            if thread:\n                # Save all messages, except the initial system message:\n                thread_messages = [m for m in result if not isinstance(m, SystemMessage)]\n                save_django_messages(cast(list[BaseMessage], thread_messages), thread=thread)\n            return result\n\n        class AgentState(TypedDict):\n            messages: Annotated[list[AnyMessage], custom_add_messages]\n            input: str | None  # noqa: A003\n            output: Any\n\n        def setup(state: AgentState):\n            system_prompt = self.get_instructions()\n            return {\"messages\": [SystemMessage(content=system_prompt)]}\n\n        def history(state: AgentState):\n            messages = thread.get_messages(include_extra_messages=True) if thread else []\n            if state[\"input\"]:\n                messages.append(HumanMessage(content=state[\"input\"]))\n\n            return {\"messages\": messages}\n\n        def retriever(state: AgentState):\n            if not self.has_rag:\n                return\n\n            retriever = self.get_history_aware_retriever()\n            # Remove the initial instructions to prevent having two SystemMessages\n            # This is necessary for compatibility with Anthropic\n            messages_to_summarize = state[\"messages\"][1:-1]\n            input_message = state[\"messages\"][-1]\n            docs = retriever.invoke({\"input\": input_message, \"history\": messages_to_summarize})\n\n            document_separator = self.get_document_separator()\n            document_prompt = self.get_document_prompt()\n\n            formatted_docs = document_separator.join(\n                format_document(doc, document_prompt) for doc in docs\n            )\n\n            system_message = state[\"messages\"][0]\n            system_message.content += (\n                f\"\\n\\n---START OF CONTEXT---\\n{formatted_docs}---END OF CONTEXT---\\n\\n\"\n            )\n\n        def agent(state: AgentState):\n            response = llm_with_tools.invoke(state[\"messages\"])\n\n            return {\"messages\": [response]}\n\n        def tool_selector(state: AgentState):\n            last_message = state[\"messages\"][-1]\n\n            if isinstance(last_message, AIMessage) and last_message.tool_calls:\n                return \"call_tool\"\n\n            return \"continue\"\n\n        def record_response(state: AgentState):\n            # Structured output must happen in the end, to avoid disabling tool calling.\n            # Tool calling + structured output is not supported by OpenAI:\n            if self.structured_output:\n                messages = state[\"messages\"]\n\n                # Change the original system prompt:\n                if isinstance(messages[0], SystemMessage):\n                    messages[0].content += \"\\nUse the chat history to produce a JSON output.\"\n\n                # Add a final message asking for JSON generation / structured output:\n                json_request_message = HumanMessage(\n                    content=\"Use the chat history to produce a JSON output.\"\n                )\n                messages.append(json_request_message)\n\n                llm_with_structured_output = self.get_structured_output_llm()\n                response = llm_with_structured_output.invoke(messages)\n            else:\n                response = state[\"messages\"][-1].content\n\n            return {\"output\": response}\n\n        workflow = StateGraph(AgentState)\n\n        workflow.add_node(\"setup\", setup)\n        workflow.add_node(\"history\", history)\n        workflow.add_node(\"retriever\", retriever)\n        workflow.add_node(\"agent\", agent)\n        workflow.add_node(\"tools\", ToolNode(tools))\n        workflow.add_node(\"respond\", record_response)\n\n        workflow.set_entry_point(\"setup\")\n        workflow.add_edge(\"setup\", \"history\")\n        workflow.add_edge(\"history\", \"retriever\")\n        workflow.add_edge(\"retriever\", \"agent\")\n        workflow.add_conditional_edges(\n            \"agent\",\n            tool_selector,\n            {\n                \"call_tool\": \"tools\",\n                \"continue\": \"respond\",\n            },\n        )\n        workflow.add_edge(\"tools\", \"agent\")\n        workflow.add_edge(\"respond\", END)\n\n        return workflow.compile()\n\n    @with_cast_id\n    def invoke(self, *args: Any, thread_id: Any | None, **kwargs: Any) -&gt; dict:\n        \"\"\"Invoke the assistant Langchain graph with the given arguments and keyword arguments.\\n\n        This is the lower-level method to run the assistant.\\n\n        The graph is created by the `as_graph` method.\\n\n\n        Args:\n            *args: Positional arguments to pass to the graph.\n                To add a new message, use a dict like `{\"input\": \"user message\"}`.\n                If thread already has a `HumanMessage` in the end, you can invoke without args.\n            thread_id (Any | None): The thread ID for the chat message history.\n                If `None`, an in-memory chat message history is used.\n            **kwargs: Keyword arguments to pass to the graph.\n\n        Returns:\n            dict: The output of the assistant graph,\n                structured like `{\"output\": \"assistant response\", \"history\": ...}`.\n        \"\"\"\n        graph = self.as_graph(thread_id)\n        config = kwargs.pop(\"config\", {})\n        config[\"max_concurrency\"] = config.pop(\"max_concurrency\", self.tool_max_concurrency)\n        return graph.invoke(*args, config=config, **kwargs)\n\n    @with_cast_id\n    def run(self, message: str, thread_id: Any | None = None, **kwargs: Any) -&gt; Any:\n        \"\"\"Run the assistant with the given message and thread ID.\\n\n        This is the higher-level method to run the assistant.\\n\n\n        Args:\n            message (str): The user message to pass to the assistant.\n            thread_id (Any | None): The thread ID for the chat message history.\n                If `None`, an in-memory chat message history is used.\n            **kwargs: Additional keyword arguments to pass to the graph.\n\n        Returns:\n            Any: The assistant response to the user message.\n        \"\"\"\n        return self.invoke(\n            {\n                \"input\": message,\n            },\n            thread_id=thread_id,\n            **kwargs,\n        )[\"output\"]\n\n    def _run_as_tool(self, message: str, **kwargs: Any) -&gt; Any:\n        return self.run(message, thread_id=None, **kwargs)\n\n    def as_tool(self, description: str) -&gt; BaseTool:\n        \"\"\"Create a tool from the assistant.\\n\n        This is useful to compose assistants.\\n\n\n        Args:\n            description (str): The description for the tool.\n\n        Returns:\n            BaseTool: A tool that runs the assistant. The tool name is this assistant's id.\n        \"\"\"\n        return StructuredTool.from_function(\n            func=self._run_as_tool,\n            name=self.id,\n            description=description,\n        )\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.id","title":"<code>id: str</code>  <code>class-attribute</code>","text":"<p>Class variable with the id of the assistant. Used to select the assistant to use.</p> <p>Must be unique across the whole Django project and match the pattern '^[a-zA-Z0-9_-]+$'.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.name","title":"<code>name: str</code>  <code>class-attribute</code>","text":"<p>Class variable with the name of the assistant. Should be a friendly name to optionally display to users.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.instructions","title":"<code>instructions: str</code>  <code>instance-attribute</code>","text":"<p>Instructions for the AI assistant knowing what to do. This is the LLM system prompt.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.model","title":"<code>model: str</code>  <code>instance-attribute</code>","text":"<p>LLM model name to use for the assistant.</p> <p>Should be a valid model name from OpenAI, because the default <code>get_llm</code> method uses OpenAI.</p> <p><code>get_llm</code> can be overridden to use a different LLM implementation.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.temperature","title":"<code>temperature: float = 1.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Temperature to use for the assistant LLM model. Defaults to <code>1.0</code>.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.tool_max_concurrency","title":"<code>tool_max_concurrency: int = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tools to run concurrently / in parallel. Defaults to <code>1</code> (no concurrency).</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.has_rag","title":"<code>has_rag: bool = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the assistant uses RAG (Retrieval-Augmented Generation) or not.</p> <p>Defaults to <code>False</code>. When True, the assistant will use a retriever to get documents to provide as context to the LLM. Additionally, the assistant class should implement the <code>get_retriever</code> method to return the retriever to use.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.structured_output","title":"<code>structured_output: Dict[str, Any] | Type[BaseModel] | Type | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Structured output to use for the assistant.</p> <p>Defaults to <code>None</code>. When not <code>None</code>, the assistant will return a structured output in the provided format. See https://python.langchain.com/v0.2/docs/how_to/structured_output/ for the available formats.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant._method_tools","title":"<code>_method_tools: Sequence[BaseTool]</code>  <code>instance-attribute</code>","text":"<p>List of <code>@method_tool</code> tools the assistant can use. Automatically set by the constructor.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant._registry","title":"<code>_registry: dict[str, type[AIAssistant]] = {}</code>  <code>class-attribute</code>","text":"<p>Registry of all AIAssistant subclasses by their id.</p> <p>Automatically populated by when a subclass is declared.</p> <p>Use <code>get_cls_registry</code> and <code>get_cls</code> to access the registry.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant._user","title":"<code>_user: Any | None = user</code>  <code>instance-attribute</code>","text":"<p>The current user the assistant is helping. A model instance.</p> <p>Set by the constructor. When API views are used, this is set to the current request user.</p> <p>Can be used in any <code>@method_tool</code> to customize behavior.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant._request","title":"<code>_request: Any | None = request</code>  <code>instance-attribute</code>","text":"<p>The current Django request the assistant was initialized with. A request instance.</p> <p>Set by the constructor.</p> <p>Can be used in any <code>@method_tool</code> to customize behavior.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant._view","title":"<code>_view: Any | None = view</code>  <code>instance-attribute</code>","text":"<p>The current Django view the assistant was initialized with. A view instance.</p> <p>Set by the constructor.</p> <p>Can be used in any <code>@method_tool</code> to customize behavior.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant._init_kwargs","title":"<code>_init_kwargs: dict[str, Any] = kwargs</code>  <code>instance-attribute</code>","text":"<p>Extra keyword arguments passed to the constructor.</p> <p>Set by the constructor.</p> <p>Can be used in any <code>@method_tool</code> to customize behavior.</p>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.__init__","title":"<code>__init__(*, user=None, request=None, view=None, **kwargs)</code>","text":"<p>Initialize the AIAssistant instance.</p> <p>Optionally set the current user, request, and view for the assistant.</p> <p>Those can be used in any <code>@method_tool</code> to customize behavior.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>Any | None</code> <p>The current user the assistant is helping. A model instance. Defaults to <code>None</code>. Stored in <code>self._user</code>.</p> <code>None</code> <code>request</code> <code>Any | None</code> <p>The current Django request the assistant was initialized with. A request instance. Defaults to <code>None</code>. Stored in <code>self._request</code>.</p> <code>None</code> <code>view</code> <code>Any | None</code> <p>The current Django view the assistant was initialized with. A view instance. Defaults to <code>None</code>. Stored in <code>self._view</code>.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Extra keyword arguments passed to the constructor. Stored in <code>self._init_kwargs</code>.</p> <code>{}</code> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def __init__(self, *, user=None, request=None, view=None, **kwargs: Any):\n    \"\"\"Initialize the AIAssistant instance.\\n\n    Optionally set the current user, request, and view for the assistant.\\n\n    Those can be used in any `@method_tool` to customize behavior.\\n\n\n    Args:\n        user (Any | None): The current user the assistant is helping. A model instance.\n            Defaults to `None`. Stored in `self._user`.\n        request (Any | None): The current Django request the assistant was initialized with.\n            A request instance. Defaults to `None`. Stored in `self._request`.\n        view (Any | None): The current Django view the assistant was initialized with.\n            A view instance. Defaults to `None`. Stored in `self._view`.\n        **kwargs: Extra keyword arguments passed to the constructor. Stored in `self._init_kwargs`.\n    \"\"\"\n\n    self._user = user\n    self._request = request\n    self._view = view\n    self._init_kwargs = kwargs\n\n    self._set_method_tools()\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_cls_registry","title":"<code>get_cls_registry()</code>  <code>classmethod</code>","text":"<p>Get the registry of AIAssistant classes.</p> <p>Returns:</p> Type Description <code>dict[str, type[AIAssistant]]</code> <p>dict[str, type[AIAssistant]]: A dictionary mapping assistant ids to their classes.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>@classmethod\ndef get_cls_registry(cls) -&gt; dict[str, type[\"AIAssistant\"]]:\n    \"\"\"Get the registry of AIAssistant classes.\n\n    Returns:\n        dict[str, type[AIAssistant]]: A dictionary mapping assistant ids to their classes.\n    \"\"\"\n    return cls._registry\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_cls","title":"<code>get_cls(assistant_id)</code>  <code>classmethod</code>","text":"<p>Get the AIAssistant class for the given assistant ID.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>The ID of the assistant to get.</p> required <p>Returns:     type[AIAssistant]: The AIAssistant subclass for the given ID.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>@classmethod\ndef get_cls(cls, assistant_id: str) -&gt; type[\"AIAssistant\"]:\n    \"\"\"Get the AIAssistant class for the given assistant ID.\n\n    Args:\n        assistant_id (str): The ID of the assistant to get.\n    Returns:\n        type[AIAssistant]: The AIAssistant subclass for the given ID.\n    \"\"\"\n    return cls.get_cls_registry()[assistant_id]\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.clear_cls_registry","title":"<code>clear_cls_registry()</code>  <code>classmethod</code>","text":"<p>Clear the registry of AIAssistant classes.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>@classmethod\ndef clear_cls_registry(cls: type[\"AIAssistant\"]) -&gt; None:\n    \"\"\"Clear the registry of AIAssistant classes.\"\"\"\n\n    cls._registry.clear()\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_instructions","title":"<code>get_instructions()</code>","text":"<p>Get the instructions for the assistant. By default, this is the <code>instructions</code> attribute.</p> <p>Override the <code>instructions</code> attribute or this method to use different instructions.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The instructions for the assistant, i.e., the LLM system prompt.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_instructions(self) -&gt; str:\n    \"\"\"Get the instructions for the assistant. By default, this is the `instructions` attribute.\\n\n    Override the `instructions` attribute or this method to use different instructions.\n\n    Returns:\n        str: The instructions for the assistant, i.e., the LLM system prompt.\n    \"\"\"\n    return self.instructions\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_model","title":"<code>get_model()</code>","text":"<p>Get the LLM model name for the assistant. By default, this is the <code>model</code> attribute.</p> <p>Used by the <code>get_llm</code> method to create the LLM instance.</p> <p>Override the <code>model</code> attribute or this method to use a different LLM model.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The LLM model name for the assistant.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_model(self) -&gt; str:\n    \"\"\"Get the LLM model name for the assistant. By default, this is the `model` attribute.\\n\n    Used by the `get_llm` method to create the LLM instance.\\n\n    Override the `model` attribute or this method to use a different LLM model.\n\n    Returns:\n        str: The LLM model name for the assistant.\n    \"\"\"\n    return self.model\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_temperature","title":"<code>get_temperature()</code>","text":"<p>Get the temperature to use for the assistant LLM model. By default, this is the <code>temperature</code> attribute, which is <code>1.0</code> by default.</p> <p>Used by the <code>get_llm</code> method to create the LLM instance.</p> <p>Override the <code>temperature</code> attribute or this method to use a different temperature.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The temperature to use for the assistant LLM model.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_temperature(self) -&gt; float:\n    \"\"\"Get the temperature to use for the assistant LLM model.\n    By default, this is the `temperature` attribute, which is `1.0` by default.\\n\n    Used by the `get_llm` method to create the LLM instance.\\n\n    Override the `temperature` attribute or this method to use a different temperature.\n\n    Returns:\n        float: The temperature to use for the assistant LLM model.\n    \"\"\"\n    return self.temperature\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_model_kwargs","title":"<code>get_model_kwargs()</code>","text":"<p>Get additional keyword arguments to pass to the LLM model constructor.</p> <p>Used by the <code>get_llm</code> method to create the LLM instance.</p> <p>Override this method to pass additional keyword arguments to the LLM model constructor.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: Additional keyword arguments to pass to the LLM model constructor.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_model_kwargs(self) -&gt; dict[str, Any]:\n    \"\"\"Get additional keyword arguments to pass to the LLM model constructor.\\n\n    Used by the `get_llm` method to create the LLM instance.\\n\n    Override this method to pass additional keyword arguments to the LLM model constructor.\n\n    Returns:\n        dict[str, Any]: Additional keyword arguments to pass to the LLM model constructor.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_llm","title":"<code>get_llm()</code>","text":"<p>Get the Langchain LLM instance for the assistant. By default, this uses the OpenAI implementation.</p> <p><code>get_model</code>, <code>get_temperature</code>, and <code>get_model_kwargs</code> are used to create the LLM instance.</p> <p>Override this method to use a different LLM implementation.</p> <p>Returns:</p> Name Type Description <code>BaseChatModel</code> <code>BaseChatModel</code> <p>The LLM instance for the assistant.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_llm(self) -&gt; BaseChatModel:\n    \"\"\"Get the Langchain LLM instance for the assistant.\n    By default, this uses the OpenAI implementation.\\n\n    `get_model`, `get_temperature`, and `get_model_kwargs` are used to create the LLM instance.\\n\n    Override this method to use a different LLM implementation.\n\n    Returns:\n        BaseChatModel: The LLM instance for the assistant.\n    \"\"\"\n    model = self.get_model()\n    temperature = self.get_temperature()\n    model_kwargs = self.get_model_kwargs()\n    return ChatOpenAI(\n        model=model,\n        temperature=temperature,\n        model_kwargs=model_kwargs,\n    )\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_structured_output_llm","title":"<code>get_structured_output_llm()</code>","text":"<p>Get the LLM model to use for the structured output.</p> <p>Returns:</p> Name Type Description <code>BaseChatModel</code> <code>Runnable</code> <p>The LLM model to use for the structured output.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_structured_output_llm(self) -&gt; Runnable:\n    \"\"\"Get the LLM model to use for the structured output.\n\n    Returns:\n        BaseChatModel: The LLM model to use for the structured output.\n    \"\"\"\n    if not self.structured_output:\n        raise ValueError(\"structured_output is not defined\")\n\n    llm = self.get_llm()\n\n    method = \"json_mode\"\n    if isinstance(llm, ChatOpenAI):\n        # When using ChatOpenAI, it's better to use json_schema method\n        # because it enables strict mode.\n        # https://platform.openai.com/docs/guides/structured-outputs\n        method = \"json_schema\"\n\n    return llm.with_structured_output(self.structured_output, method=method)\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_tools","title":"<code>get_tools()</code>","text":"<p>Get the list of method tools the assistant can use. By default, this is the <code>_method_tools</code> attribute, which are all <code>@method_tool</code>s.</p> <p>Override and call super to add additional tools, such as any langchain_community tools.</p> <p>Returns:</p> Type Description <code>Sequence[BaseTool]</code> <p>Sequence[BaseTool]: The list of tools the assistant can use.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_tools(self) -&gt; Sequence[BaseTool]:\n    \"\"\"Get the list of method tools the assistant can use.\n    By default, this is the `_method_tools` attribute, which are all `@method_tool`s.\\n\n    Override and call super to add additional tools,\n    such as [any langchain_community tools](https://python.langchain.com/v0.2/docs/integrations/tools/).\n\n    Returns:\n        Sequence[BaseTool]: The list of tools the assistant can use.\n    \"\"\"\n    return self._method_tools\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_document_separator","title":"<code>get_document_separator()</code>","text":"<p>Get the RAG document separator to use in the prompt. Only used when <code>has_rag=True</code>.</p> <p>Defaults to <code>\"\\n\\n\"</code>, which is the Langchain default.</p> <p>Override this method to use a different separator.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>a separator for documents in the prompt.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_document_separator(self) -&gt; str:\n    \"\"\"Get the RAG document separator to use in the prompt. Only used when `has_rag=True`.\\n\n    Defaults to `\"\\\\n\\\\n\"`, which is the Langchain default.\\n\n    Override this method to use a different separator.\n\n    Returns:\n        str: a separator for documents in the prompt.\n    \"\"\"\n    return DEFAULT_DOCUMENT_SEPARATOR\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_document_prompt","title":"<code>get_document_prompt()</code>","text":"<p>Get the PromptTemplate template to use when rendering RAG documents in the prompt. Only used when <code>has_rag=True</code>.</p> <p>Defaults to <code>PromptTemplate.from_template(\"{page_content}\")</code>, which is the Langchain default.</p> <p>Override this method to use a different template.</p> <p>Returns:</p> Name Type Description <code>PromptTemplate</code> <code>PromptTemplate</code> <p>a prompt template for RAG documents.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_document_prompt(self) -&gt; PromptTemplate:\n    \"\"\"Get the PromptTemplate template to use when rendering RAG documents in the prompt.\n    Only used when `has_rag=True`.\\n\n    Defaults to `PromptTemplate.from_template(\"{page_content}\")`, which is the Langchain default.\\n\n    Override this method to use a different template.\n\n    Returns:\n        PromptTemplate: a prompt template for RAG documents.\n    \"\"\"\n    return DEFAULT_DOCUMENT_PROMPT\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_retriever","title":"<code>get_retriever()</code>","text":"<p>Get the RAG retriever to use for fetching documents.</p> <p>Must be implemented by subclasses when <code>has_rag=True</code>.</p> <p>Returns:</p> Name Type Description <code>BaseRetriever</code> <code>BaseRetriever</code> <p>the RAG retriever to use for fetching documents.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_retriever(self) -&gt; BaseRetriever:\n    \"\"\"Get the RAG retriever to use for fetching documents.\\n\n    Must be implemented by subclasses when `has_rag=True`.\\n\n\n    Returns:\n        BaseRetriever: the RAG retriever to use for fetching documents.\n    \"\"\"\n    raise NotImplementedError(\n        f\"Override the get_retriever with your implementation at {self.__class__.__name__}\"\n    )\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_contextualize_prompt","title":"<code>get_contextualize_prompt()</code>","text":"<p>Get the contextualize prompt template for the assistant.</p> <p>This is used when <code>has_rag=True</code> and there are previous messages in the thread. Since the latest user question might reference the chat history, the LLM needs to generate a new standalone question, and use that question to query the retriever for relevant documents.</p> <p>By default, this is a prompt that asks the LLM to reformulate the latest user question without the chat history.</p> <p>Override this method to use a different contextualize prompt.</p> <p>See <code>get_history_aware_retriever</code> for how this prompt is used.</p> <p>Returns:</p> Name Type Description <code>ChatPromptTemplate</code> <code>ChatPromptTemplate</code> <p>The contextualize prompt template for the assistant.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_contextualize_prompt(self) -&gt; ChatPromptTemplate:\n    \"\"\"Get the contextualize prompt template for the assistant.\\n\n    This is used when `has_rag=True` and there are previous messages in the thread.\n    Since the latest user question might reference the chat history,\n    the LLM needs to generate a new standalone question,\n    and use that question to query the retriever for relevant documents.\\n\n    By default, this is a prompt that asks the LLM to\n    reformulate the latest user question without the chat history.\\n\n    Override this method to use a different contextualize prompt.\\n\n    See `get_history_aware_retriever` for how this prompt is used.\\n\n\n    Returns:\n        ChatPromptTemplate: The contextualize prompt template for the assistant.\n    \"\"\"\n    contextualize_q_system_prompt = (\n        \"Given a chat history and the latest user question \"\n        \"which might reference context in the chat history, \"\n        \"formulate a standalone question which can be understood \"\n        \"without the chat history. Do NOT answer the question, \"\n        \"just reformulate it if needed and otherwise return it as is.\"\n    )\n    return ChatPromptTemplate.from_messages(\n        [\n            (\"system\", contextualize_q_system_prompt),\n            # TODO: make history key configurable?\n            MessagesPlaceholder(\"history\"),\n            # TODO: make input key configurable?\n            (\"human\", \"{input}\"),\n        ]\n    )\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.get_history_aware_retriever","title":"<code>get_history_aware_retriever()</code>","text":"<p>Get the history-aware retriever Langchain chain for the assistant.</p> <p>This is used when <code>has_rag=True</code> to fetch documents based on the chat history.</p> <p>By default, this is a chain that checks if there is chat history, and if so, it uses the chat history to generate a new standalone question to query the retriever for relevant documents.</p> <p>When there is no chat history, it just passes the input to the retriever.</p> <p>Override this method to use a different history-aware retriever chain.</p> <p>Read more about the history-aware retriever in the Langchain docs.</p> <p>Returns:</p> Type Description <code>Runnable[dict, RetrieverOutput]</code> <p>Runnable[dict, RetrieverOutput]: a history-aware retriever Langchain chain.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def get_history_aware_retriever(self) -&gt; Runnable[dict, RetrieverOutput]:\n    \"\"\"Get the history-aware retriever Langchain chain for the assistant.\\n\n    This is used when `has_rag=True` to fetch documents based on the chat history.\\n\n    By default, this is a chain that checks if there is chat history,\n    and if so, it uses the chat history to generate a new standalone question\n    to query the retriever for relevant documents.\\n\n    When there is no chat history, it just passes the input to the retriever.\\n\n    Override this method to use a different history-aware retriever chain.\n\n    Read more about the history-aware retriever in the\n    [Langchain docs](https://python.langchain.com/v0.2/docs/how_to/qa_chat_history_how_to/).\n\n    Returns:\n        Runnable[dict, RetrieverOutput]: a history-aware retriever Langchain chain.\n    \"\"\"\n    llm = self.get_llm()\n    retriever = self.get_retriever()\n    prompt = self.get_contextualize_prompt()\n\n    # Based on create_history_aware_retriever:\n    return RunnableBranch(\n        (\n            lambda x: not x.get(\"history\", False),  # pyright: ignore[reportAttributeAccessIssue]\n            # If no chat history, then we just pass input to retriever\n            (lambda x: x[\"input\"]) | retriever,\n        ),\n        # If chat history, then we pass inputs to LLM chain, then to retriever\n        prompt | llm | StrOutputParser() | retriever,\n    )\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.as_graph","title":"<code>as_graph(thread_id=None)</code>","text":"<p>Create the Langchain graph for the assistant.</p> <p>This graph is an agent that supports chat history, tool calling, and RAG (if <code>has_rag=True</code>).</p> <p><code>as_graph</code> uses many other methods to create the graph for the assistant. Prefer to override the other methods to customize the graph for the assistant. Only override this method if you need to customize the graph at a lower level.</p> <p>Parameters:</p> Name Type Description Default <code>thread_id</code> <code>Any | None</code> <p>The thread ID for the chat message history. If <code>None</code>, an in-memory chat message history is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Runnable[dict, dict]</code> <p>the compiled graph</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>@with_cast_id\ndef as_graph(self, thread_id: Any | None = None) -&gt; Runnable[dict, dict]:\n    \"\"\"Create the Langchain graph for the assistant.\\n\n    This graph is an agent that supports chat history, tool calling, and RAG (if `has_rag=True`).\\n\n    `as_graph` uses many other methods to create the graph for the assistant.\n    Prefer to override the other methods to customize the graph for the assistant.\n    Only override this method if you need to customize the graph at a lower level.\n\n    Args:\n        thread_id (Any | None): The thread ID for the chat message history.\n            If `None`, an in-memory chat message history is used.\n\n    Returns:\n        the compiled graph\n    \"\"\"\n    from django_ai_assistant.models import Thread\n\n    llm = self.get_llm()\n    tools = self.get_tools()\n    llm_with_tools = llm.bind_tools(tools) if tools else llm\n    if thread_id:\n        thread = Thread.objects.get(id=thread_id)\n    else:\n        thread = None\n\n    def custom_add_messages(left: list[BaseMessage], right: list[BaseMessage]):\n        result = add_messages(left, right)  # type: ignore\n        if thread:\n            # Save all messages, except the initial system message:\n            thread_messages = [m for m in result if not isinstance(m, SystemMessage)]\n            save_django_messages(cast(list[BaseMessage], thread_messages), thread=thread)\n        return result\n\n    class AgentState(TypedDict):\n        messages: Annotated[list[AnyMessage], custom_add_messages]\n        input: str | None  # noqa: A003\n        output: Any\n\n    def setup(state: AgentState):\n        system_prompt = self.get_instructions()\n        return {\"messages\": [SystemMessage(content=system_prompt)]}\n\n    def history(state: AgentState):\n        messages = thread.get_messages(include_extra_messages=True) if thread else []\n        if state[\"input\"]:\n            messages.append(HumanMessage(content=state[\"input\"]))\n\n        return {\"messages\": messages}\n\n    def retriever(state: AgentState):\n        if not self.has_rag:\n            return\n\n        retriever = self.get_history_aware_retriever()\n        # Remove the initial instructions to prevent having two SystemMessages\n        # This is necessary for compatibility with Anthropic\n        messages_to_summarize = state[\"messages\"][1:-1]\n        input_message = state[\"messages\"][-1]\n        docs = retriever.invoke({\"input\": input_message, \"history\": messages_to_summarize})\n\n        document_separator = self.get_document_separator()\n        document_prompt = self.get_document_prompt()\n\n        formatted_docs = document_separator.join(\n            format_document(doc, document_prompt) for doc in docs\n        )\n\n        system_message = state[\"messages\"][0]\n        system_message.content += (\n            f\"\\n\\n---START OF CONTEXT---\\n{formatted_docs}---END OF CONTEXT---\\n\\n\"\n        )\n\n    def agent(state: AgentState):\n        response = llm_with_tools.invoke(state[\"messages\"])\n\n        return {\"messages\": [response]}\n\n    def tool_selector(state: AgentState):\n        last_message = state[\"messages\"][-1]\n\n        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n            return \"call_tool\"\n\n        return \"continue\"\n\n    def record_response(state: AgentState):\n        # Structured output must happen in the end, to avoid disabling tool calling.\n        # Tool calling + structured output is not supported by OpenAI:\n        if self.structured_output:\n            messages = state[\"messages\"]\n\n            # Change the original system prompt:\n            if isinstance(messages[0], SystemMessage):\n                messages[0].content += \"\\nUse the chat history to produce a JSON output.\"\n\n            # Add a final message asking for JSON generation / structured output:\n            json_request_message = HumanMessage(\n                content=\"Use the chat history to produce a JSON output.\"\n            )\n            messages.append(json_request_message)\n\n            llm_with_structured_output = self.get_structured_output_llm()\n            response = llm_with_structured_output.invoke(messages)\n        else:\n            response = state[\"messages\"][-1].content\n\n        return {\"output\": response}\n\n    workflow = StateGraph(AgentState)\n\n    workflow.add_node(\"setup\", setup)\n    workflow.add_node(\"history\", history)\n    workflow.add_node(\"retriever\", retriever)\n    workflow.add_node(\"agent\", agent)\n    workflow.add_node(\"tools\", ToolNode(tools))\n    workflow.add_node(\"respond\", record_response)\n\n    workflow.set_entry_point(\"setup\")\n    workflow.add_edge(\"setup\", \"history\")\n    workflow.add_edge(\"history\", \"retriever\")\n    workflow.add_edge(\"retriever\", \"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        tool_selector,\n        {\n            \"call_tool\": \"tools\",\n            \"continue\": \"respond\",\n        },\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n    workflow.add_edge(\"respond\", END)\n\n    return workflow.compile()\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.invoke","title":"<code>invoke(*args, thread_id, **kwargs)</code>","text":"<p>Invoke the assistant Langchain graph with the given arguments and keyword arguments.</p> <p>This is the lower-level method to run the assistant.</p> <p>The graph is created by the <code>as_graph</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>Positional arguments to pass to the graph. To add a new message, use a dict like <code>{\"input\": \"user message\"}</code>. If thread already has a <code>HumanMessage</code> in the end, you can invoke without args.</p> <code>()</code> <code>thread_id</code> <code>Any | None</code> <p>The thread ID for the chat message history. If <code>None</code>, an in-memory chat message history is used.</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass to the graph.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The output of the assistant graph, structured like <code>{\"output\": \"assistant response\", \"history\": ...}</code>.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>@with_cast_id\ndef invoke(self, *args: Any, thread_id: Any | None, **kwargs: Any) -&gt; dict:\n    \"\"\"Invoke the assistant Langchain graph with the given arguments and keyword arguments.\\n\n    This is the lower-level method to run the assistant.\\n\n    The graph is created by the `as_graph` method.\\n\n\n    Args:\n        *args: Positional arguments to pass to the graph.\n            To add a new message, use a dict like `{\"input\": \"user message\"}`.\n            If thread already has a `HumanMessage` in the end, you can invoke without args.\n        thread_id (Any | None): The thread ID for the chat message history.\n            If `None`, an in-memory chat message history is used.\n        **kwargs: Keyword arguments to pass to the graph.\n\n    Returns:\n        dict: The output of the assistant graph,\n            structured like `{\"output\": \"assistant response\", \"history\": ...}`.\n    \"\"\"\n    graph = self.as_graph(thread_id)\n    config = kwargs.pop(\"config\", {})\n    config[\"max_concurrency\"] = config.pop(\"max_concurrency\", self.tool_max_concurrency)\n    return graph.invoke(*args, config=config, **kwargs)\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.run","title":"<code>run(message, thread_id=None, **kwargs)</code>","text":"<p>Run the assistant with the given message and thread ID.</p> <p>This is the higher-level method to run the assistant.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The user message to pass to the assistant.</p> required <code>thread_id</code> <code>Any | None</code> <p>The thread ID for the chat message history. If <code>None</code>, an in-memory chat message history is used.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the graph.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The assistant response to the user message.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>@with_cast_id\ndef run(self, message: str, thread_id: Any | None = None, **kwargs: Any) -&gt; Any:\n    \"\"\"Run the assistant with the given message and thread ID.\\n\n    This is the higher-level method to run the assistant.\\n\n\n    Args:\n        message (str): The user message to pass to the assistant.\n        thread_id (Any | None): The thread ID for the chat message history.\n            If `None`, an in-memory chat message history is used.\n        **kwargs: Additional keyword arguments to pass to the graph.\n\n    Returns:\n        Any: The assistant response to the user message.\n    \"\"\"\n    return self.invoke(\n        {\n            \"input\": message,\n        },\n        thread_id=thread_id,\n        **kwargs,\n    )[\"output\"]\n</code></pre>"},{"location":"reference/assistants-ref/#django_ai_assistant.helpers.assistants.AIAssistant.as_tool","title":"<code>as_tool(description)</code>","text":"<p>Create a tool from the assistant.</p> <p>This is useful to compose assistants.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>The description for the tool.</p> required <p>Returns:</p> Name Type Description <code>BaseTool</code> <code>BaseTool</code> <p>A tool that runs the assistant. The tool name is this assistant's id.</p> Source code in <code>django_ai_assistant/helpers/assistants.py</code> <pre><code>def as_tool(self, description: str) -&gt; BaseTool:\n    \"\"\"Create a tool from the assistant.\\n\n    This is useful to compose assistants.\\n\n\n    Args:\n        description (str): The description for the tool.\n\n    Returns:\n        BaseTool: A tool that runs the assistant. The tool name is this assistant's id.\n    \"\"\"\n    return StructuredTool.from_function(\n        func=self._run_as_tool,\n        name=self.id,\n        description=description,\n    )\n</code></pre>"},{"location":"reference/models-ref/","title":"django_ai_assistant.models","text":""},{"location":"reference/models-ref/#django_ai_assistant.models.Thread","title":"<code>Thread</code>","text":"<p>               Bases: <code>Model</code></p> <p>Thread model. A thread is a collection of messages between a user and the AI assistant. Also called conversation or session.</p> Source code in <code>django_ai_assistant/models.py</code> <pre><code>class Thread(models.Model):\n    \"\"\"Thread model. A thread is a collection of messages between a user and the AI assistant.\n    Also called conversation or session.\"\"\"\n\n    id: Any  # noqa: A003\n    messages: Manager[\"Message\"]\n    name = models.CharField(max_length=255, blank=True)\n    \"\"\"Name of the thread. Can be blank.\"\"\"\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL,\n        on_delete=models.SET_NULL,\n        related_name=\"ai_assistant_threads\",\n        null=True,\n    )\n    \"\"\"User who created the thread. Can be null. Set to null/None when user is deleted.\"\"\"\n    assistant_id = models.CharField(max_length=255, blank=True)\n    \"\"\"Associated assistant ID. Can be empty.\"\"\"\n    created_at = models.DateTimeField(auto_now_add=True)\n    \"\"\"Date and time when the thread was created.\n    Automatically set when the thread is created.\"\"\"\n    updated_at = models.DateTimeField(auto_now=True)\n    \"\"\"Date and time when the thread was last updated.\n    Automatically set when the thread is updated.\"\"\"\n\n    class Meta:\n        verbose_name = \"Thread\"\n        verbose_name_plural = \"Threads\"\n        ordering = (\"-created_at\",)\n        indexes = (Index(F(\"created_at\").desc(), name=\"thread_created_at_desc\"),)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return the name of the thread as the string representation of the thread.\"\"\"\n        return self.name\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the thread like '&lt;Thread name&gt;'\"\"\"\n        return f\"&lt;Thread {self.name}&gt;\"\n\n    def get_messages(self, include_extra_messages: bool = False) -&gt; list[BaseMessage]:\n        \"\"\"\n        Get Langchain messages objects from the thread.\n\n        Args:\n            include_extra_messages (bool): Whether to include non-chat messages (like tool calls).\n\n        Returns:\n            list[BaseMessage]: List of messages\n        \"\"\"\n\n        messages = messages_from_dict(\n            cast(\n                Sequence[dict[str, BaseMessage]],\n                Message.objects.filter(thread=self)\n                .order_by(\"created_at\")\n                .values_list(\"message\", flat=True),\n            )\n        )\n        if not include_extra_messages:\n            messages = [\n                m\n                for m in messages\n                if isinstance(m, HumanMessage | ChatMessage)\n                or (isinstance(m, AIMessage) and not m.tool_calls)\n            ]\n        return cast(list[BaseMessage], messages)\n</code></pre>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.name","title":"<code>name = models.CharField(max_length=255, blank=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Name of the thread. Can be blank.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.created_by","title":"<code>created_by = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.SET_NULL, related_name='ai_assistant_threads', null=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>User who created the thread. Can be null. Set to null/None when user is deleted.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.assistant_id","title":"<code>assistant_id = models.CharField(max_length=255, blank=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Associated assistant ID. Can be empty.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.created_at","title":"<code>created_at = models.DateTimeField(auto_now_add=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Date and time when the thread was created. Automatically set when the thread is created.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.updated_at","title":"<code>updated_at = models.DateTimeField(auto_now=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Date and time when the thread was last updated. Automatically set when the thread is updated.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.__str__","title":"<code>__str__()</code>","text":"<p>Return the name of the thread as the string representation of the thread.</p> Source code in <code>django_ai_assistant/models.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return the name of the thread as the string representation of the thread.\"\"\"\n    return self.name\n</code></pre>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the thread like '' Source code in <code>django_ai_assistant/models.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the thread like '&lt;Thread name&gt;'\"\"\"\n    return f\"&lt;Thread {self.name}&gt;\"\n</code></pre>"},{"location":"reference/models-ref/#django_ai_assistant.models.Thread.get_messages","title":"<code>get_messages(include_extra_messages=False)</code>","text":"<p>Get Langchain messages objects from the thread.</p> <p>Parameters:</p> Name Type Description Default <code>include_extra_messages</code> <code>bool</code> <p>Whether to include non-chat messages (like tool calls).</p> <code>False</code> <p>Returns:</p> Type Description <code>list[BaseMessage]</code> <p>list[BaseMessage]: List of messages</p> Source code in <code>django_ai_assistant/models.py</code> <pre><code>def get_messages(self, include_extra_messages: bool = False) -&gt; list[BaseMessage]:\n    \"\"\"\n    Get Langchain messages objects from the thread.\n\n    Args:\n        include_extra_messages (bool): Whether to include non-chat messages (like tool calls).\n\n    Returns:\n        list[BaseMessage]: List of messages\n    \"\"\"\n\n    messages = messages_from_dict(\n        cast(\n            Sequence[dict[str, BaseMessage]],\n            Message.objects.filter(thread=self)\n            .order_by(\"created_at\")\n            .values_list(\"message\", flat=True),\n        )\n    )\n    if not include_extra_messages:\n        messages = [\n            m\n            for m in messages\n            if isinstance(m, HumanMessage | ChatMessage)\n            or (isinstance(m, AIMessage) and not m.tool_calls)\n        ]\n    return cast(list[BaseMessage], messages)\n</code></pre>"},{"location":"reference/models-ref/#django_ai_assistant.models.Message","title":"<code>Message</code>","text":"<p>               Bases: <code>Model</code></p> <p>Message model. A message is a text that is part of a thread. A message can be sent by a user or the AI assistant.</p> <p>The message data is stored as a JSON field called <code>message</code>.</p> Source code in <code>django_ai_assistant/models.py</code> <pre><code>class Message(models.Model):\n    \"\"\"Message model. A message is a text that is part of a thread.\n    A message can be sent by a user or the AI assistant.\\n\n    The message data is stored as a JSON field called `message`.\"\"\"\n\n    id: Any  # noqa: A003\n    thread = models.ForeignKey(Thread, on_delete=models.CASCADE, related_name=\"messages\")\n    \"\"\"Thread to which the message belongs.\"\"\"\n    thread_id: Any\n    message = models.JSONField()\n    \"\"\"Message content. This is a serialized Langchain `BaseMessage` that was serialized\n    with `message_to_dict` and can be deserialized with `messages_from_dict`.\"\"\"\n    created_at = models.DateTimeField(auto_now_add=True)\n    \"\"\"Date and time when the message was created.\n    Automatically set when the message is created.\"\"\"\n\n    class Meta:\n        verbose_name = \"Message\"\n        verbose_name_plural = \"Messages\"\n        ordering = (\"created_at\",)\n        indexes = (Index(F(\"created_at\"), name=\"message_created_at\"),)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return internal message data from `message` attribute\n        as the string representation of the message.\"\"\"\n        return json.dumps(self.message)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the string representation of the message like '&lt;Message id at thread_id&gt;'\"\"\"\n        return f\"&lt;Message {self.id} at {self.thread_id}&gt;\"\n</code></pre>"},{"location":"reference/models-ref/#django_ai_assistant.models.Message.thread","title":"<code>thread = models.ForeignKey(Thread, on_delete=models.CASCADE, related_name='messages')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Thread to which the message belongs.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Message.message","title":"<code>message = models.JSONField()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Message content. This is a serialized Langchain <code>BaseMessage</code> that was serialized with <code>message_to_dict</code> and can be deserialized with <code>messages_from_dict</code>.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Message.created_at","title":"<code>created_at = models.DateTimeField(auto_now_add=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Date and time when the message was created. Automatically set when the message is created.</p>"},{"location":"reference/models-ref/#django_ai_assistant.models.Message.__str__","title":"<code>__str__()</code>","text":"<p>Return internal message data from <code>message</code> attribute as the string representation of the message.</p> Source code in <code>django_ai_assistant/models.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return internal message data from `message` attribute\n    as the string representation of the message.\"\"\"\n    return json.dumps(self.message)\n</code></pre>"},{"location":"reference/models-ref/#django_ai_assistant.models.Message.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the string representation of the message like '' Source code in <code>django_ai_assistant/models.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the string representation of the message like '&lt;Message id at thread_id&gt;'\"\"\"\n    return f\"&lt;Message {self.id} at {self.thread_id}&gt;\"\n</code></pre>"},{"location":"reference/use-cases-ref/","title":"django_ai_assistant.helpers.use_cases","text":""},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.get_assistant_cls","title":"<code>get_assistant_cls(assistant_id, user, request=None)</code>","text":"<p>Get assistant class by id.</p> <p>Uses <code>AI_ASSISTANT_CAN_RUN_ASSISTANT_FN</code> permission to check if user can run the assistant.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>Assistant id to get</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     type[AIAssistant]: Assistant class with the given id Raises:     AIAssistantNotDefinedError: If assistant with the given id is not found     AIUserNotAllowedError: If user is not allowed to use the assistant</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def get_assistant_cls(\n    assistant_id: str,\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; type[AIAssistant]:\n    \"\"\"Get assistant class by id.\\n\n    Uses `AI_ASSISTANT_CAN_RUN_ASSISTANT_FN` permission to check if user can run the assistant.\n\n    Args:\n        assistant_id (str): Assistant id to get\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        type[AIAssistant]: Assistant class with the given id\n    Raises:\n        AIAssistantNotDefinedError: If assistant with the given id is not found\n        AIUserNotAllowedError: If user is not allowed to use the assistant\n    \"\"\"\n    if assistant_id not in AIAssistant.get_cls_registry():\n        raise AIAssistantNotDefinedError(f\"Assistant with id={assistant_id} not found\")\n    assistant_cls = AIAssistant.get_cls(assistant_id)\n    if not can_run_assistant(\n        assistant_cls=assistant_cls,\n        user=user,\n        request=request,\n    ):\n        raise AIUserNotAllowedError(\"User is not allowed to use this assistant\")\n    return assistant_cls\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.get_single_assistant_info","title":"<code>get_single_assistant_info(assistant_id, user, request=None)</code>","text":"<p>Get assistant info id. Returns a dictionary with the assistant id and name.</p> <p>Uses <code>AI_ASSISTANT_CAN_RUN_ASSISTANT_FN</code> permission to check if user can see the assistant.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>Assistant id to get</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     dict[str, str]: dict like <code>{\"id\": \"personal_ai\", \"name\": \"Personal AI\"}</code> Raises:     AIAssistantNotDefinedError: If assistant with the given id is not found     AIUserNotAllowedError: If user is not allowed to see the assistant</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def get_single_assistant_info(\n    assistant_id: str,\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; dict[str, str]:\n    \"\"\"Get assistant info id. Returns a dictionary with the assistant id and name.\\n\n    Uses `AI_ASSISTANT_CAN_RUN_ASSISTANT_FN` permission to check if user can see the assistant.\n\n    Args:\n        assistant_id (str): Assistant id to get\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        dict[str, str]: dict like `{\"id\": \"personal_ai\", \"name\": \"Personal AI\"}`\n    Raises:\n        AIAssistantNotDefinedError: If assistant with the given id is not found\n        AIUserNotAllowedError: If user is not allowed to see the assistant\n    \"\"\"\n    assistant_cls = get_assistant_cls(assistant_id, user, request)\n\n    return {\n        \"id\": assistant_id,\n        \"name\": assistant_cls.name,\n    }\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.get_assistants_info","title":"<code>get_assistants_info(user, request=None)</code>","text":"<p>Get all assistants info. Returns a list of dictionaries with the assistant id and name.</p> <p>Uses <code>AI_ASSISTANT_CAN_RUN_ASSISTANT_FN</code> permission to check the assistants the user can see, and returns only the ones the user can see.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     list[dict[str, str]]: List of dicts like <code>[{\"id\": \"personal_ai\", \"name\": \"Personal AI\"}, ...]</code></p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def get_assistants_info(\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; list[dict[str, str]]:\n    \"\"\"Get all assistants info. Returns a list of dictionaries with the assistant id and name.\\n\n    Uses `AI_ASSISTANT_CAN_RUN_ASSISTANT_FN` permission to check the assistants the user can see,\n    and returns only the ones the user can see.\n\n    Args:\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        list[dict[str, str]]: List of dicts like `[{\"id\": \"personal_ai\", \"name\": \"Personal AI\"}, ...]`\n    \"\"\"\n    assistant_info_list = []\n    for assistant_id in AIAssistant.get_cls_registry().keys():\n        try:\n            info = get_single_assistant_info(assistant_id, user, request)\n            assistant_info_list.append(info)\n        except AIUserNotAllowedError:\n            continue\n    return assistant_info_list\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.create_message","title":"<code>create_message(assistant_id, thread, user, content, request=None)</code>","text":"<p>Create a message in a thread, and right after runs the assistant to get the AI response.</p> <p>Uses <code>AI_ASSISTANT_CAN_RUN_ASSISTANT_FN</code> permission to check if user can run the assistant.</p> <p>Uses <code>AI_ASSISTANT_CAN_CREATE_MESSAGE_FN</code> permission to check if user can create a message in the thread.</p> <p>Parameters:</p> Name Type Description Default <code>assistant_id</code> <code>str</code> <p>Assistant id to use to get the AI response</p> required <code>thread</code> <code>Thread</code> <p>Thread where to create the message</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>content</code> <code>Any</code> <p>Message content, usually a string</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     dict: The output of the assistant,         structured like <code>{\"output\": \"assistant response\", \"history\": ...}</code> Raises:     AIUserNotAllowedError: If user is not allowed to create messages in the thread</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def create_message(\n    assistant_id: str,\n    thread: Thread,\n    user: Any,\n    content: Any,\n    request: HttpRequest | None = None,\n) -&gt; dict:\n    \"\"\"Create a message in a thread, and right after runs the assistant to get the AI response.\\n\n    Uses `AI_ASSISTANT_CAN_RUN_ASSISTANT_FN` permission to check if user can run the assistant.\\n\n    Uses `AI_ASSISTANT_CAN_CREATE_MESSAGE_FN` permission to check if user can create a message in the thread.\n\n    Args:\n        assistant_id (str): Assistant id to use to get the AI response\n        thread (Thread): Thread where to create the message\n        user (Any): Current user\n        content (Any): Message content, usually a string\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        dict: The output of the assistant,\n            structured like `{\"output\": \"assistant response\", \"history\": ...}`\n    Raises:\n        AIUserNotAllowedError: If user is not allowed to create messages in the thread\n    \"\"\"\n    assistant_cls = get_assistant_cls(assistant_id, user, request)\n\n    if not can_create_message(thread=thread, user=user, request=request):\n        raise AIUserNotAllowedError(\"User is not allowed to create messages in this thread\")\n\n    # TODO: Check if we can separate the message creation from the chain invoke\n    assistant = assistant_cls(user=user, request=request)\n    assistant_message = assistant.invoke(\n        {\"input\": content},\n        thread_id=thread.id,\n    )\n    return assistant_message\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.create_thread","title":"<code>create_thread(name, user, assistant_id=None, request=None)</code>","text":"<p>Create a thread.</p> <p>Uses <code>AI_ASSISTANT_CAN_CREATE_THREAD_FN</code> permission to check if user can create a thread.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Thread name</p> required <code>assistant_id</code> <code>str | None</code> <p>Assistant ID to associate the thread with. If empty or None, the thread is not associated with any assistant.</p> <code>None</code> <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     Thread: Created thread model instance Raises:     AIUserNotAllowedError: If user is not allowed to create threads</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def create_thread(\n    name: str,\n    user: Any,\n    assistant_id: str | None = None,\n    request: HttpRequest | None = None,\n) -&gt; Thread:\n    \"\"\"Create a thread.\\n\n    Uses `AI_ASSISTANT_CAN_CREATE_THREAD_FN` permission to check if user can create a thread.\n\n    Args:\n        name (str): Thread name\n        assistant_id (str | None): Assistant ID to associate the thread with.\n            If empty or None, the thread is not associated with any assistant.\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        Thread: Created thread model instance\n    Raises:\n        AIUserNotAllowedError: If user is not allowed to create threads\n    \"\"\"\n    if not can_create_thread(user=user, request=request):\n        raise AIUserNotAllowedError(\"User is not allowed to create threads\")\n\n    thread = Thread.objects.create(name=name, created_by=user, assistant_id=assistant_id or \"\")\n    return thread\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.get_single_thread","title":"<code>get_single_thread(thread_id, user, request=None)</code>","text":"<p>Get a single thread by id.</p> <p>Uses <code>AI_ASSISTANT_CAN_VIEW_THREAD_FN</code> permission to check if user can view the thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread_id</code> <code>str</code> <p>Thread id to get</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     Thread: Thread model instance Raises:     AIUserNotAllowedError: If user is not allowed to view the thread</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def get_single_thread(\n    thread_id: Any,\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; Thread:\n    \"\"\"Get a single thread by id.\\n\n    Uses `AI_ASSISTANT_CAN_VIEW_THREAD_FN` permission to check if user can view the thread.\n\n    Args:\n        thread_id (str): Thread id to get\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        Thread: Thread model instance\n    Raises:\n        AIUserNotAllowedError: If user is not allowed to view the thread\n    \"\"\"\n    thread = Thread.objects.get(id=thread_id)\n\n    if not can_view_thread(thread=thread, user=user, request=request):\n        raise AIUserNotAllowedError(\"User is not allowed to view this thread\")\n\n    return thread\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.get_threads","title":"<code>get_threads(user, assistant_id=None, request=None)</code>","text":"<p>Get all threads for the user.</p> <p>Uses <code>AI_ASSISTANT_CAN_VIEW_THREAD_FN</code> permission to check the threads the user can see, and returns only the ones the user can see.</p> <p>Parameters:</p> Name Type Description Default <code>user</code> <code>Any</code> <p>Current user</p> required <code>assistant_id</code> <code>str | None</code> <p>Assistant ID to filter threads by. If empty or None, all threads for the user are returned.</p> <code>None</code> <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     list[Thread]: List of thread model instances</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def get_threads(\n    user: Any,\n    assistant_id: str | None = None,\n    request: HttpRequest | None = None,\n) -&gt; list[Thread]:\n    \"\"\"Get all threads for the user.\\n\n    Uses `AI_ASSISTANT_CAN_VIEW_THREAD_FN` permission to check the threads the user can see,\n    and returns only the ones the user can see.\n\n    Args:\n        user (Any): Current user\n        assistant_id (str | None): Assistant ID to filter threads by.\n            If empty or None, all threads for the user are returned.\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        list[Thread]: List of thread model instances\n    \"\"\"\n    threads = Thread.objects.filter(created_by=user)\n\n    if assistant_id:\n        threads = threads.filter(assistant_id=assistant_id)\n\n    return list(\n        threads.filter(\n            id__in=[\n                thread.id\n                for thread in threads\n                if can_view_thread(thread=thread, user=user, request=request)\n            ]\n        )\n    )\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.update_thread","title":"<code>update_thread(thread, name, user, request=None)</code>","text":"<p>Update thread name.</p> <p>Uses <code>AI_ASSISTANT_CAN_UPDATE_THREAD_FN</code> permission to check if user can update the thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>Thread model instance to update</p> required <code>name</code> <code>str</code> <p>New thread name</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     Thread: Updated thread model instance Raises:     AIUserNotAllowedError: If user is not allowed to update the thread</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def update_thread(\n    thread: Thread,\n    name: str,\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; Thread:\n    \"\"\"Update thread name.\\n\n    Uses `AI_ASSISTANT_CAN_UPDATE_THREAD_FN` permission to check if user can update the thread.\n\n    Args:\n        thread (Thread): Thread model instance to update\n        name (str): New thread name\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        Thread: Updated thread model instance\n    Raises:\n        AIUserNotAllowedError: If user is not allowed to update the thread\n    \"\"\"\n    if not can_update_thread(thread=thread, user=user, request=request):\n        raise AIUserNotAllowedError(\"User is not allowed to update this thread\")\n\n    thread.name = name\n    thread.save()\n    return thread\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.delete_thread","title":"<code>delete_thread(thread, user, request=None)</code>","text":"<p>Delete a thread.</p> <p>Uses <code>AI_ASSISTANT_CAN_DELETE_THREAD_FN</code> permission to check if user can delete the thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>Thread model instance to delete</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Raises:     AIUserNotAllowedError: If user is not allowed to delete the thread</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def delete_thread(\n    thread: Thread,\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; None:\n    \"\"\"Delete a thread.\\n\n    Uses `AI_ASSISTANT_CAN_DELETE_THREAD_FN` permission to check if user can delete the thread.\n\n    Args:\n        thread (Thread): Thread model instance to delete\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Raises:\n        AIUserNotAllowedError: If user is not allowed to delete the thread\n    \"\"\"\n    if not can_delete_thread(thread=thread, user=user, request=request):\n        raise AIUserNotAllowedError(\"User is not allowed to delete this thread\")\n\n    thread.delete()\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.get_thread_messages","title":"<code>get_thread_messages(thread, user, request=None)</code>","text":"<p>Get all messages in a thread.</p> <p>Uses <code>AI_ASSISTANT_CAN_VIEW_THREAD_FN</code> permission to check if user can view the thread.</p> <p>Parameters:</p> Name Type Description Default <code>thread</code> <code>Thread</code> <p>Thread model instance to get messages from</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Returns:     list[BaseMessage]: List of message instances</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def get_thread_messages(\n    thread: Thread,\n    user: Any,\n    request: HttpRequest | None = None,\n) -&gt; list[BaseMessage]:\n    \"\"\"Get all messages in a thread.\\n\n    Uses `AI_ASSISTANT_CAN_VIEW_THREAD_FN` permission to check if user can view the thread.\n\n    Args:\n        thread (Thread): Thread model instance to get messages from\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Returns:\n        list[BaseMessage]: List of message instances\n    \"\"\"\n    # TODO: have more permissions for threads? View thread permission?\n    if user != thread.created_by:\n        raise AIUserNotAllowedError(\"User is not allowed to view messages in this thread\")\n\n    return thread.get_messages(include_extra_messages=False)\n</code></pre>"},{"location":"reference/use-cases-ref/#django_ai_assistant.helpers.use_cases.delete_message","title":"<code>delete_message(message, user, request=None)</code>","text":"<p>Delete a message.</p> <p>Uses <code>AI_ASSISTANT_CAN_DELETE_MESSAGE_FN</code> permission to check if user can delete the message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>Message model instance to delete</p> required <code>user</code> <code>Any</code> <p>Current user</p> required <code>request</code> <code>HttpRequest | None</code> <p>Current request, if any</p> <code>None</code> <p>Raises:     AIUserNotAllowedError: If user is not allowed to delete the message</p> Source code in <code>django_ai_assistant/helpers/use_cases.py</code> <pre><code>def delete_message(\n    message: Message,\n    user: Any,\n    request: HttpRequest | None = None,\n):\n    \"\"\"Delete a message.\\n\n    Uses `AI_ASSISTANT_CAN_DELETE_MESSAGE_FN` permission to check if user can delete the message.\n\n    Args:\n        message (Message): Message model instance to delete\n        user (Any): Current user\n        request (HttpRequest | None): Current request, if any\n    Raises:\n        AIUserNotAllowedError: If user is not allowed to delete the message\n    \"\"\"\n    if not can_delete_message(message=message, user=user, request=request):\n        raise AIUserNotAllowedError(\"User is not allowed to delete this message\")\n\n    return message.delete()\n</code></pre>"}]}